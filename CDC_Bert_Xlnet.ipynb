{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CDC_Bert_Xlnet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-YpbdxbYfx6Q"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/CDC_Bert_Xlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BqLg04UoIU6",
        "colab_type": "text"
      },
      "source": [
        "## Link with github project folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7T6jfH2s1wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/acmilannesta/Bert-embedding\n",
        "!pip install keras-bert\n",
        "# !git clone https://github.com/acmilannesta/eda_nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "306WTFXmGPA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "51c0dd33-8591-4e20-e66f-94816b723129"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import choice\n",
        "import re, os, gc\n",
        "import codecs\n",
        "import boto3\n",
        "from keras.layers import *\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps, get_custom_objects\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['AWS_SHARED_CREDENTIALS_FILE'] = 'AWS.txt'\n",
        "s3 = boto3.Session(profile_name='default').client('s3')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhh2j65PSAgB",
        "colab_type": "code",
        "outputId": "8ec02e51-575c-499a-9131-da43fe99e587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlLlzYSm9pR",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset\n",
        "1. Add event weight\n",
        "2. Reassign event code (0-47) for all\n",
        "3. Add binary indicators for top 5 words by each event type\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQK47tDfHL-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('Bert-embedding/CDC/train.csv')\n",
        "test = pd.read_csv('Bert-embedding/CDC/test.csv')\n",
        "# Event weight\n",
        "wt = pd.DataFrame(train.event.value_counts()/len(train)).rename(columns={'event':'weight'})\n",
        "wt['event'] = wt.index\n",
        "train = train.merge(wt, how='left', on='event')\n",
        "# Reassign eventcode\n",
        "train['event_idx'] = train.event.map({y:x for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "# Assign weight freqency\n",
        "train['wt_freq'] = np.where(train.weight<0.01, 1, np.where(train.weight<0.05, 2, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDY40RM-n3jq",
        "colab_type": "text"
      },
      "source": [
        "## EDA: Word frequencies by Event Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdyXVucz8h0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Worclouds for top 20 events \n",
        "from itertools import groupby \n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "train['text_grouped'] = train.groupby('event')['text'].transform(lambda x: ' '.join(x))\n",
        "x = train[['event', 'text_grouped', 'weight']].drop_duplicates('event').sort_values('weight', ascending=False)\n",
        "x.reset_index(drop=True, inplace=True)\n",
        "\n",
        "f, ax = plt.subplots(10, 2, figsize=(30,30))\n",
        "\n",
        "for seq in range(20):\n",
        "    string = x.loc[seq, 'text_grouped'].split(' ')\n",
        "    counts = [(len(list(c)),i) for i,c in groupby(sorted(string)) if len(i)>3 and i!='WORK'] \n",
        "    counts_dict = {x[1]:x[0] for x in counts}\n",
        "\n",
        "    wordcloud = WordCloud(height=200, width=200, margin=0, collocations=False).generate_from_frequencies(counts_dict)\n",
        "    \n",
        "    ax[seq//2, seq%2].imshow(wordcloud, interpolation='bilinear')\n",
        "    ax[seq//2, seq%2].set_title('event:'+str(x.loc[seq, 'event']) #+' weight:'+str(round(x.loc[seq, 'weight'], 2)),\n",
        "                    ,fontsize=16, color='white')\n",
        "    ax[seq//2, seq%2].set_axis_off()\n",
        "    ax[seq//2, seq%2].margins(x=0, y=0)\n",
        "    plt.tight_layout(w_pad=0.025)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DygvhLqzW9b6",
        "colab_type": "text"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO11GkeioDer",
        "colab_type": "text"
      },
      "source": [
        "## Download BERT checkpoint and dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw-2XW8Geus",
        "colab_type": "code",
        "outputId": "5474ca2f-39b6-49af-d887-ac933c1c8def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-16 21:48:21--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 2a00:1450:4013:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  90.6MB/s    in 4.3s    \n",
            "\n",
            "2019-11-16 21:48:25 (90.6 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54DOZcMKE4sl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "2407d1d9-4933-473b-ab23-c79a4043d101"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-16 20:46:35--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 2a00:1450:4013:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248381879 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_uncased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "wwm_uncased_L-24_H- 100%[===================>]   1.16G  75.4MB/s    in 16s     \n",
            "\n",
            "2019-11-16 20:46:51 (76.4 MB/s) - ‘wwm_uncased_L-24_H-1024_A-16.zip’ saved [1248381879/1248381879]\n",
            "\n",
            "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
            "   creating: wwm_uncased_L-24_H-1024_A-16/\n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSngi92Ym2az",
        "colab_type": "text"
      },
      "source": [
        "## Parameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJhMtLYVGl0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 142 #@param {type:'slider', min:50, max:300, step:1}\n",
        "BATCH_SIZE = 16 #@param {type:'slider', min:8, max:32, step:8}\n",
        "NUM_EPOCHS = 3\n",
        "NUM_CLASSES = 48\n",
        "LR = 5e-5\n",
        "MIN_LR = 0\n",
        "OUTPUT_TRAIN = 'train_bert_ipredcv15_oof.csv'\n",
        "OUTPUT_TEST = 'test_base_multidrop.npy'\n",
        "config_path = 'uncased_L-12_H-768_A-12/bert_config.json' #@param ['uncased_L-12_H-768_A-12/bert_config.json', 'wwm_uncased_L-24_H-1024_A-16/bert_config.json', '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/bert_config.json']\n",
        "checkpoint_path = 'uncased_L-12_H-768_A-12/bert_model.ckpt' #@param ['uncased_L-12_H-768_A-12/bert_model.ckpt', 'wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt', '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/model.ckpt']\n",
        "dict_path = 'uncased_L-12_H-768_A-12/vocab.txt' #@param ['uncased_L-12_H-768_A-12/vocab.txt', 'wwm_uncased_L-24_H-1024_A-16/vocab.txt', '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/vocab.txt']\n",
        "# config_path1 = '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/bert_config.json'\n",
        "# checkpoint_path1 = '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/model.ckpt'\n",
        "# dict_path1 = '/content/drive/My Drive/biobert_pretrain_output_all_notes_150000/vocab.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCK8Uhcjmvmc",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HC_n547GuYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "# token_dict1 = {}\n",
        "# with codecs.open(dict_path1, 'r', 'utf8') as reader:\n",
        "#     for line in reader:\n",
        "#         token = line.strip()\n",
        "#         token_dict1[token] = len(token_dict1)\n",
        "# tokenizer1 = Tokenizer(token_dict1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLKGXJQEZ3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df, branch='training'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    indices, indices1 = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        ids, segments = tokenizer.encode(data_df.loc[i, 'text'])\n",
        "        # ids1, segments1 = tokenizer1.encode(data_df.loc[i, 'text'])\n",
        "        indices.append(ids)\n",
        "        # indices1.append(ids1)\n",
        "    aux = data_df[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x)))\n",
        "    if branch=='training':\n",
        "        targets = data_df['event_idx'] \n",
        "        return indices, np.array(targets), np.array(aux)\n",
        "    else:\n",
        "        return indices, np.array(aux)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN2Yb16Jmqxk",
        "colab_type": "text"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl-tLqJGHXij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq_padding(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = max(L)\n",
        "    return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n",
        "\n",
        "class data_generator:\n",
        "    def __init__(self, data, batch_size=BATCH_SIZE, branch='train'):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.branch = branch\n",
        "        self.steps = len(self.data) // self.batch_size\n",
        "        if len(self.data) % self.batch_size != 0:\n",
        "            self.steps += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if self.branch == 'train':\n",
        "                np.random.shuffle(self.data)\n",
        "            for i in range(self.steps):\n",
        "                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "                X1 = seq_padding([x[0] for x in d])           \n",
        "                X2 = np.zeros_like(X1)\n",
        "                # X3 = seq_padding([x[1] for x in d])           \n",
        "                # X4 = np.zeros_like(X3)\n",
        "                if self.branch == 'test':\n",
        "                    aux = np.array([x[1] for x in d])\n",
        "                    yield [X1, X2, aux]\n",
        "                else:\n",
        "                    Y = np.array([x[1] for x in d])\n",
        "                    aux = np.array([x[2] for x in d])\n",
        "                    yield [X1, X2, aux], Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjR06fNGIIdC",
        "colab_type": "text"
      },
      "source": [
        "##Model Assemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnY5nGsHgHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_build(len_train):\n",
        "    global NUM_CLASSES\n",
        "    global BATCH_SIZE\n",
        "    global NUM_EPOCHS\n",
        "    global MIN_LR\n",
        "    global LR\n",
        "    global MAXLEN\n",
        "\n",
        "    bert_model = load_trained_model_from_checkpoint(\n",
        "        config_path,\n",
        "        checkpoint_path,\n",
        "        seq_len = MAXLEN,\n",
        "        trainable=True\n",
        "    )\n",
        "\n",
        "    # clinic_model = load_trained_model_from_checkpoint(\n",
        "    #     config_path1,\n",
        "    #     checkpoint_path1,\n",
        "    #     seq_len = MAXLEN,\n",
        "    #     trainable=True\n",
        "    # )\n",
        "\n",
        "    x1_in = Input(shape=(None,))\n",
        "    x2_in = Input(shape=(None,))\n",
        "    # x3_in = Input(shape=(None,))\n",
        "    # x4_in = Input(shape=(None,))\n",
        "    aux_in = Input(shape=(2, ))\n",
        "\n",
        "    inputs = bert_model([x1_in, x2_in])\n",
        "    bert = Lambda(lambda x: x[:, 0])(inputs)\n",
        "\n",
        "    # inputs1 = clinic_model([x3_in, x4_in])\n",
        "    # clinic = Lambda(lambda x: x[:, 0])(inputs1)\n",
        "    # combo = Maximum()([bert, clinic])\n",
        "    outputs = []\n",
        "    for _ in range(5):\n",
        "        tmp = Dropout(0.5)(bert)\n",
        "        tmp = concatenate([tmp, aux_in])\n",
        "        tmp = Dense(NUM_CLASSES, activation='softmax')(tmp)\n",
        "        outputs.append(tmp)\n",
        "\n",
        "    # dense = concatenate([d_avg, aux_in])\n",
        "    # outputs = Dense(NUM_CLASSES, activation='softmax')(dense)\n",
        "    outputs = Average()(outputs)\n",
        "    model = Model([x1_in, x2_in, aux_in], outputs)\n",
        "\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer=AdamWarmup(\n",
        "            decay_steps=decay_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            lr=LR,\n",
        "            min_lr=MIN_LR,\n",
        "            ),\n",
        "        metrics=['sparse_categorical_accuracy']\n",
        "    )\n",
        "    del bert_model\n",
        "    gc.collect()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0fWMbNYa7hu",
        "colab_type": "text"
      },
      "source": [
        "## Batchwise evaluation callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg3lanyguZHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntervalPrediction(Callback):\n",
        "\n",
        "    def __init__(self, test_data, pred, pred1, nsplits, fold):\n",
        "        super(Callback, self).__init__()\n",
        "        self.test_data = test_data\n",
        "        self.pred = pred\n",
        "        self.nsplits = nsplits\n",
        "        self.fold = fold\n",
        "        self.pred1 = pred1\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # self.seen += logs.get('num_steps', 1)\n",
        "        if epoch == 2:\n",
        "            self.pred += self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1) * 0.3 / self.nsplits\n",
        "            model_file = 'model-oof-'+str(self.fold)+'-'+str(epoch+1)+'.h5'\n",
        "            self.model.save('model.h5')\n",
        "            s3.upload_file('model.h5', 'acmilannesta', 'ipred/'+model_file)\n",
        "        if epoch == 3:\n",
        "            tmp = self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1) / self.nsplits\n",
        "            self.pred += tmp * 0.7\n",
        "            self.pred1 = tmp\n",
        "            model_file = 'model-oof-'+str(self.fold)+'-'+str(epoch+1)+'.h5'\n",
        "            self.model.save('model.h5')\n",
        "            s3.upload_file('model.h5', 'acmilannesta', 'ipred/'+model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31l7HU8ZHsAe",
        "colab_type": "code",
        "outputId": "c0f4c80b-8b89-4a87-d3b5-b3a7c6cf7cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "test_indices, test_aux = convert_data(test, branch='test')\n",
        "# pred = np.zeros((len(test), NUM_CLASSES))\n",
        "pred = np.load(OUTPUT_TEST)\n",
        "kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=0)\n",
        "idx = [x for x in kf.split(train, train.wt_freq)]\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx[3:], 4):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    tr_x, tr_y, tr_aux = convert_data(tr)\n",
        "    val_x, val_y, val_aux = convert_data(val)\n",
        "    model = model_build(len_train=len(tr_x))\n",
        "    train_D = data_generator(list(zip(tr_x, tr_y, tr_aux)))\n",
        "    valid_D = data_generator(list(zip(val_x, val_y, val_aux)), branch='valid')\n",
        "    test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n",
        "    # ipred = IntervalPrediction(test_data=test_D, pred=pred, nsplits=kf.get_n_splits(), fold=i, pred1=pred1)\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        # callbacks = [ipred]\n",
        "    )\n",
        "    oof_pred = model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    # train_aug.loc[val_idx, 'oof_pred'] = np.argmax(oof_pred, 1)\n",
        "    print('oof - {:} f1_score - {:.4f}'.format(i, f1_score(val_y, np.argmax(oof_pred, 1), average='weighted')))\n",
        "\n",
        "    pred += model.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n",
        "    np.save(OUTPUT_TEST, pred)\n",
        "    s3.upload_file(Filename=OUTPUT_TEST, Bucket='acmilannesta', Key='base/'+OUTPUT_TEST)\n",
        "\n",
        "    model_file = 'model-oof-'+str(i)+'.h5'\n",
        "    model.save('model.h5')\n",
        "    s3.upload_file(Filename='model.h5', Bucket='acmilannesta', Key='base/'+model_file)\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/3\n",
            "7698/7698 [==============================] - 1033s 134ms/step - loss: 0.7851 - sparse_categorical_accuracy: 0.7782\n",
            "Epoch 2/3\n",
            "7698/7698 [==============================] - 1009s 131ms/step - loss: 0.3720 - sparse_categorical_accuracy: 0.8836\n",
            "Epoch 3/3\n",
            "7698/7698 [==============================] - 1007s 131ms/step - loss: 0.2436 - sparse_categorical_accuracy: 0.9203\n",
            "1925/1925 [==============================] - 55s 29ms/step\n",
            "oof - 4 f1_score - 0.8826\n",
            "4742/4742 [==============================] - 132s 28ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 540/123166 [00:00<00:22, 5392.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 123166/123166 [00:20<00:00, 5977.42it/s]\n",
            "100%|██████████| 30790/30790 [00:05<00:00, 5956.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7698/7698 [==============================] - 1032s 134ms/step - loss: 0.7700 - sparse_categorical_accuracy: 0.7840\n",
            "Epoch 2/3\n",
            "7698/7698 [==============================] - 1019s 132ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.8852\n",
            "Epoch 3/3\n",
            "7698/7698 [==============================] - 1021s 133ms/step - loss: 0.2414 - sparse_categorical_accuracy: 0.9213\n",
            "1925/1925 [==============================] - 58s 30ms/step\n",
            "oof - 5 f1_score - 0.8789\n",
            "4742/4742 [==============================] - 133s 28ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 588/123163 [00:00<00:20, 5878.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 6\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 123163/123163 [00:20<00:00, 6080.81it/s]\n",
            "100%|██████████| 30793/30793 [00:05<00:00, 5960.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65tc6F7LW8IE",
        "colab_type": "code",
        "outputId": "92a13013-0fb1-4385-fb07-ca53d8aea78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n",
        "pred_e4 = np.zeros((len(test), NUM_CLASSES))\n",
        "for i in range(2, 16):\n",
        "    print('Fold - {:} - Prediction'.format(i))\n",
        "    load_file = 'ipred/model-oof-' + str(i) + '-4.h5'\n",
        "    s3.download_file(Bucket='acmilannesta', Key=load_file, Filename='model.h5')\n",
        "    loaded = load_model('model.h5', custom_objects=get_custom_objects())\n",
        "    pred_e4 += loaded.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n",
        "    del loaded\n",
        "    !rm model.h5\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fr7t8hplhlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model_build(len(test_D))\n",
        "model.load_weights(model.h5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKp8L-hvbQXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['event'] = np.argmax(pred, 1)\n",
        "test.event = test.event.map({x: y for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "test.to_csv('solution.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5otYpFI9WUaH",
        "colab_type": "code",
        "outputId": "c735a4e2-0c6d-47eb-9ea2-2597af6bec77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# fit on whole training set and make preds on testing set\n",
        "train_x, train_y, train_aux = convert_data(train)\n",
        "model = model_build(len(train_x))\n",
        "train_D = data_generator(list(zip(train_x, train_y, train_aux)))\n",
        "model.fit_generator(    \n",
        "    train_D.__iter__(),\n",
        "    steps_per_epoch=len(train_D),\n",
        "    epochs=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "test_indices, test_aux = convert_data(test, branch='test')\n",
        "test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n",
        "pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n",
        "test['bert_uncased'] = np.argmax(pred, 1)\n",
        "test.to_csv('drive/My Drive/CDC Model/oof/test_bert_uncased_whole.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75864/75864 [00:17<00:00, 4390.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2frFKaglPXeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('drive/My Drive/CDC Model/M18.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TBwAPLDdSZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_D = data_generator(list(zip(test_indices, test_aux)), branch='test')\n",
        "# pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n",
        "test['event'] = np.argmax(pred, 1)\n",
        "test['event'] = test.bert_clinic.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "test.drop('bert_clinic', 1).to_csv('solution.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jffzGhUxMgvV",
        "colab_type": "text"
      },
      "source": [
        "## Pseudo labeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sAivZIJR54U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('/content/drive/My Drive/CDC Model/solution_m14.csv')\n",
        "test['event_idx'] = test.event.map({y:x for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "# test['event_idx'] = test.bert_uncased\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_66TD1bMgNf",
        "colab_type": "code",
        "outputId": "8006fe71-098d-454f-ce3c-aa0649024cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tr, val = train_test_split(train, test_size=0.2, random_state=0)\n",
        "tr_pseudo, val_pseudo = train_test_split(tr, test_size = 0.2, random_state = 1)\n",
        "tr_pseudo = pd.concat([tr_pseudo, test, val])\n",
        "# pseudo = pd.concat([train[['text', 'age', 'sex', 'event_idx']], test[['text', 'age', 'sex', 'event_idx']]], 0)\n",
        "\n",
        "tr_pseudo_x, tr_pseudo_y, tr_pseudo_aux = convert_data(tr_pseudo)\n",
        "val_pseudo_x, val_pseudo_y, val_pseudo_aux = convert_data(val_pseudo)\n",
        "pseudo_test_x, pseudo_test_aux = convert_data(test, branch='testing')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75864/75864 [00:18<00:00, 4066.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE4mRzhIOjBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_D = data_generator(list(zip(tr_pseudo_x, tr_pseudo_y, tr_pseudo_aux)))\n",
        "test_D = data_generator(list(zip(pseudo_test_x, pseudo_test_aux)), branch='test')\n",
        "valid_D = data_generator(list(zip(val_pseudo_x, val_pseudo_y, val_pseudo_aux)), branch='valid')\n",
        "ival = IntervalEvaluation(validation_data=valid_D, label=val_pseudo_y, interval = len(train_D))\n",
        "model = model_build(len(train_D))\n",
        "model.fit_generator(    \n",
        "    train_D.__iter__(),\n",
        "    steps_per_epoch=len(train_D),\n",
        "    epochs=2,\n",
        "    callbacks = [ival]\n",
        ")\n",
        "pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n",
        "test['event'] = np.argmax(pred, 1)\n",
        "test['event'] = test.event.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "test[['text', 'sex', 'age', 'event']].to_csv('solution.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Kp5B8lXZHB",
        "colab_type": "text"
      },
      "source": [
        "# xlnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFSP_E6DXcMd",
        "colab_type": "code",
        "outputId": "ddb62b4a-ee2b-4c1b-b4c1-87e9e19bcf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n",
        "!unzip cased_L-12_H-768_A-12.zip\n",
        "# !wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
        "# !unzip cased_L-24_H-1024_A-16.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-02 22:39:22--  https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433638019 (414M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 413.55M  41.4MB/s    in 10s     \n",
            "\n",
            "2019-11-02 22:39:34 (40.3 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [433638019/433638019]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: xlnet_cased_L-12_H-768_A-12/\n",
            "  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.index  \n",
            "  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.data-00000-of-00001  \n",
            "  inflating: xlnet_cased_L-12_H-768_A-12/spiece.model  \n",
            "  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_model.ckpt.meta  \n",
            "  inflating: xlnet_cased_L-12_H-768_A-12/xlnet_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JyDeVwgXl1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras_xlnet\n",
        "import os\n",
        "from keras_xlnet import Tokenizer, load_trained_model_from_checkpoint, ATTENTION_TYPE_BI, ATTENTION_TYPE_UNI\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkJ0oUohX8pB",
        "colab_type": "text"
      },
      "source": [
        "## Parameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6WEeg-FX-P6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = 'xlnet_cased_L-12_H-768_A-12' \n",
        "MEMLEN=512\n",
        "BATCH_SIZE=16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPie-ovaz6l",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmC5x11c9Gg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import XLNetTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCf5wiIa6O4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = Tokenizer(os.path.join(checkpoint_path, 'spiece.model'))\n",
        "def convert_data(data_df):\n",
        "    # data_df.sample(frac=1, random_state=0)\n",
        "    data_df.reset_index(drop=True, inplace=True)\n",
        "    global tokenizer\n",
        "    indices = []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        ids = tokenizer.encode(data_df.loc[i, 'text'])\n",
        "        indices.append(ids)\n",
        "    targets = data_df['event_idx']\n",
        "    aux = data_df[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x)))\n",
        "    return indices, np.array(targets), np.array(aux)\n",
        "\n",
        "tr, val = train_test_split(train, test_size=0.2, random_state=0)\n",
        "tr_x, tr_y, tr_aux = convert_data(tr)\n",
        "val_x, val_y, val_aux = convert_data(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZS6_5XzrUA",
        "colab_type": "text"
      },
      "source": [
        "## Data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlSZWoZJRWr6",
        "colab_type": "code",
        "outputId": "cbc7f92f-a269-441d-e678-67cc92d94dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer.decode(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3bN05o_R7Ti",
        "colab_type": "code",
        "outputId": "43f402ad-f055-4aed-b05e-e133380e3dd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "t.SYM_PAD"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3JpOJehR3lS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.encode(train.loc[0, 'text'], add_special_tokens=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mm8b_YTObQ1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq_padding(X, padding=0):\n",
        "  L = [len(x) for x in X]\n",
        "  ML = max(L)\n",
        "  return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n",
        "\n",
        "def seq_seg(X):\n",
        "  seg = [[tokenizer.SYM_UNK]*(len(x)-1)+[tokenizer.SYM_EOS] for x in X]\n",
        "  ML = max([len(x) for x in X])    \n",
        "  return np.array([np.concatenate([x, [tokenizer.SYM_SEP] * (ML - len(x))]) if len(x) < ML else x for x in seg])\n",
        "\n",
        "def seq_mask(X):\n",
        "  mask = [[tokenizer.SYM_UNK]*len(x) for x in X]\n",
        "  ML = max([len(x) for x in X])    \n",
        "  return np.array([np.concatenate([x, [tokenizer.SYM_BOS] * (ML - len(x))]) if len(x) < ML else x for x in mask])\n",
        "\n",
        "\n",
        "class data_generator:\n",
        "  def __init__(self, data, batch_size=BATCH_SIZE, memlen=MEMLEN, branch='train'):\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.memlen = memlen\n",
        "    self.branch = branch\n",
        "    self.steps = len(self.data) // self.batch_size\n",
        "    if len(self.data) % self.batch_size != 0:\n",
        "        self.steps += 1\n",
        "  def __len__(self):\n",
        "    return self.steps\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "        if self.branch=='train':\n",
        "            np.random.shuffle(self.data)\n",
        "        for i in range(self.steps):\n",
        "            d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "            X1 = seq_padding([x[0] for x in d])\n",
        "            # segments\n",
        "            X2 = seq_seg(([x[0] for x in d]))\n",
        "            # memories\n",
        "            X3 = np.array([self.memlen for i in range(len(d))])\n",
        "            # masks\n",
        "            # X4 = seq_mask(([x[0] for x in d]))\n",
        "            Y = np.array([x[1] for x in d])\n",
        "            aux = np.array([x[2] for x in d])\n",
        "            yield [X1, X2, X3, aux], Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YpbdxbYfx6Q",
        "colab_type": "text"
      },
      "source": [
        "## Model Assemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLoDPW_fYQII",
        "colab_type": "code",
        "outputId": "695d8b27-d900-4db0-c2ca-e9b533312854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "xlnet_model = load_trained_model_from_checkpoint(\n",
        "    config_path=os.path.join(checkpoint_path, 'xlnet_config.json'),\n",
        "    checkpoint_path=os.path.join(checkpoint_path, 'xlnet_model.ckpt'),\n",
        "    batch_size=BATCH_SIZE, #16\n",
        "    memory_len=MEMLEN, #512\n",
        "    target_len=142, #128\n",
        "    in_train_phase=False,\n",
        "    attention_type=ATTENTION_TYPE_BI,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE9_d2diZjyL",
        "colab_type": "code",
        "outputId": "b2d65725-ad1e-40ca-d115-1bb5021dafad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "x1_in = Input(shape=(None,))\n",
        "x2_in = Input(shape=(None,))\n",
        "x3_in = Input(shape=(1,))\n",
        "# x4_in = Input(shape=(None,))\n",
        "aux_in = Input(shape=(2, ))\n",
        "\n",
        "x = xlnet_model([x1_in, x2_in, x3_in])\n",
        "x = Lambda(lambda x: x[:, 0])(x)\n",
        "x = concatenate([x, aux_in])\n",
        "p = Dense(48, activation='softmax')(x)\n",
        "\n",
        "model = Model([x1_in, x2_in, x3_in, aux_in], p)\n",
        "\n",
        "decay_steps, warmup_steps = calc_train_steps(\n",
        "    len(tr_x),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=2\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    # optimizer=Adam(1e-4),\n",
        "    optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, learning_rate=1e-4, min_lr=1e-6),\n",
        "    metrics= ['sparse_categorical_accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJJkN4CRkAaH",
        "colab_type": "text"
      },
      "source": [
        "## Batchwise evaluation callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_cSeaqJkCk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IntervalEvaluation(Callback):\n",
        "    def __init__(self, validation_data, label, weight, interval=3000):\n",
        "        # super(Callback, self).__init__()\n",
        "        self.seen = 0\n",
        "        self.interval = interval\n",
        "        self.validation_data = validation_data\n",
        "        self.label = label\n",
        "        self.weight = weight\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.seen += logs.get('num_steps', 1)\n",
        "        if self.seen % self.interval == 0:\n",
        "            y_pred = self.model.predict_generator(self.validation_data.__iter__(), len(self.validation_data))\n",
        "            score = f1_score(self.label, np.argmax(y_pred, 1), average='weighted', sample_weight=self.weight)\n",
        "            print(\" - interval evaluation - batch: {:d} - score: {:.4f}\".format(self.seen, score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVyQmw-jqDR",
        "colab_type": "code",
        "outputId": "5c9a6ff9-db3e-4229-b42a-93056dff75c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_D = data_generator(list(zip(tr_x, tr_y, tr_aux)))\n",
        "valid_D = data_generator(list(zip(val_x, val_y, val_aux)), branch='valid')\n",
        "ival = IntervalEvaluation(validation_data=valid_D, label=val_y, weight=val_wt, interval = len(train_D))\n",
        "model.fit_generator(\n",
        "    train_D.__iter__(),\n",
        "    steps_per_epoch=len(train_D),\n",
        "    epochs=2,\n",
        "    # validation_data=valid_D.__iter__(),\n",
        "    # validation_steps=len(valid_D),\n",
        "    callbacks = [ival]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/2\n",
            "7697/7698 [============================>.] - ETA: 1s - loss: 0.7548 - sparse_categorical_accuracy: 0.7835 - interval evaluation - batch: 7698 - score: 0.9037\n",
            "7698/7698 [==============================] - 12201s 2s/step - loss: 0.7548 - sparse_categorical_accuracy: 0.7835\n",
            "Epoch 2/2\n",
            "3641/7698 [=============>................] - ETA: 1:38:33 - loss: 0.4291 - sparse_categorical_accuracy: 0.8653WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/2\n",
            "7697/7698 [============================>.] - ETA: 1s - loss: 0.7548 - sparse_categorical_accuracy: 0.7835 - interval evaluation - batch: 7698 - score: 0.9037\n",
            "7698/7698 [==============================] - 12201s 2s/step - loss: 0.7548 - sparse_categorical_accuracy: 0.7835\n",
            "Epoch 2/2\n",
            "7697/7698 [============================>.] - ETA: 1s - loss: 0.3980 - sparse_categorical_accuracy: 0.8731 - interval evaluation - batch: 15396 - score: 0.9122\n",
            "7698/7698 [==============================] - 12191s 2s/step - loss: 0.3979 - sparse_categorical_accuracy: 0.8731\n",
            " - interval evaluation - batch: 15396 - score: 0.9122\n",
            "7698/7698 [==============================] - 12191s 2s/step - loss: 0.3979 - sparse_categorical_accuracy: 0.8731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabe5c79c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabe5c79c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LF-wzfvc9J-",
        "colab_type": "code",
        "outputId": "30c3d4c7-8151-49ce-8ba8-655e6ec86c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_pred = model.predict_generator(valid_D.__iter__(), len(valid_D))\n",
        "f1_score(val_y, np.argmax(y_pred, 1), average='weighted', sample_weight=val_wt)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8694244178850293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLM_FwWD0BBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0bZX1sxq-CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('drive/My Drive/CDC Model/xlnet_base.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW8deZGmTK6u",
        "colab_type": "text"
      },
      "source": [
        "## Prediction on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHW8FC5uS_JM",
        "colab_type": "code",
        "outputId": "2627ea1f-7e2a-4faa-e707-35258ca291a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_indices = []\n",
        "for i in tqdm(range(len(test))):\n",
        "    ids = tokenizer.encode(test.loc[i, 'text'])\n",
        "    ids.extend([tokenizer.SYM_SEP, tokenizer.SYM_CLS])\n",
        "    test_indices.append(ids)\n",
        "test_aux = np.array(test[['age', 'sex']].apply(lambda x: (x - min(x)) / (max(x)-min(x))))\n",
        "\n",
        "class test_generator:\n",
        "  def __init__(self, data, batch_size=BATCH_SIZE, memlen=MEMLEN):\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.memlen = memlen\n",
        "    self.steps = len(self.data) // self.batch_size\n",
        "    if len(self.data) % self.batch_size != 0:\n",
        "        self.steps += 1\n",
        "  def __len__(self):\n",
        "    return self.steps\n",
        "  def __iter__(self):\n",
        "    while True:\n",
        "        for i in range(self.steps):\n",
        "            d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "            X1 = seq_padding([x[0] for x in d])\n",
        "            # segments\n",
        "            X2 = seq_seg([x[0] for x in d])\n",
        "            # memories\n",
        "            X3 = np.array([self.memlen for i in range(len(d))])\n",
        "            # masks\n",
        "            # X4 = seq_mask(([x[0] for x in d]))\n",
        "            # Y = np.array([x[1] for x in d])\n",
        "            aux = np.array([x[1] for x in d])\n",
        "            yield [X1, X2, X3, aux]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75864/75864 [00:08<00:00, 8644.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DnbdftZTHBB",
        "colab_type": "code",
        "outputId": "2b7168a9-2e16-4ee3-80ba-a18b62344f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_D = test_generator(list(zip(test_indices, test_aux)))\n",
        "pred = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1)\n",
        "test['event'] = np.argmax(pred, 1)\n",
        "test['event'] = test.event.map({x:y for x, y in enumerate(np.sort(train.event.unique()))})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4742/4742 [==============================] - 2407s 508ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6LYwEEMx70Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('solution.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}