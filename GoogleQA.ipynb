{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleQA.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/GoogleQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82bieSpLcuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "f5d726d4-b198-4755-aca7-bddaf3b08f96"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 10 13:31:02 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErzYVOxLtWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1645aa5d-af95-43f2-d3a3-4fc95a0f0e66"
      },
      "source": [
        "!pip install keras-bert\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.17.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.5)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=74505e1b8503dd9e11ba5ce26fde3b4ff699fcb31572e0bce462f04b38d442a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=d0a06b0bb888f2e4494ae83632b7ee7e04ae46c494c901a1e0778286cd0ded05\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=65dfcea0bad0702585d387d248515b2d0182f6ca1245031d0cad6855528787fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=079fdb0e05888d3fa4001b83811900b2d9553adf1497bfd542b39d4d5e06fc36\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=cf49836720c8bbdd836beea509f0f42d23da3b4420695923ea6776d60d064547\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=c225ea0ecbba96a4471853d7eddfad3f19609bc7f4a96528e45fd6b5f4262c68\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=4af6acaa7a29abab772d9e2184519726ff359fb76f61cce41d7adf363cdb7f9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=4f63fde51bcc47d70dfc09a36fc905923a98aab7b5465ca5c9dd791ab5918f12\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0\n",
            "--2019-12-10 13:31:31--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 2607:f8b0:400e:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  79.4MB/s    in 4.9s    \n",
            "\n",
            "2019-12-10 13:31:36 (79.4 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBrwK8dwLyw8",
        "colab_type": "code",
        "outputId": "f5a73f69-8aab-4091-a3fd-187631b70789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaDm4NWvLp_J",
        "colab_type": "code",
        "outputId": "99426d28-43b0-4ee7-ca62-148988b0997a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, gc\n",
        "import codecs\n",
        "from keras.layers import *\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Model, load_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps, get_custom_objects\n",
        "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xGojs4dL6S4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'drive/My Drive/GoogleQA/Data/'\n",
        "train = pd.read_csv(data_path+'train.csv')\n",
        "test = pd.read_csv(data_path+'test.csv')\n",
        "sub = pd.read_csv(data_path+'sample_submission.csv')\n",
        "\n",
        "# df_aux = pd.concat([train[['host', 'category']], test[['host', 'category']]], 0)\n",
        "# aux1 = pd.get_dummies(df_aux['category'], drop_first=True)\n",
        "# aux2 = pd.get_dummies(df_aux['host'].apply(lambda x: x.split('.')[0]), drop_first=True)\n",
        "# aux = pd.concat([aux1, aux2], 1)\n",
        "# train_aux = pd.concat([train, aux.iloc[:len(train), ]], 1)\n",
        "# test_aux =  pd.concat([test, aux.iloc[len(train):, ]], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH_AbgWGwUIi",
        "colab_type": "text"
      },
      "source": [
        "##Chunknize answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYMg8WaswSx0",
        "colab_type": "code",
        "outputId": "71be4872-a871-4e8a-d678-80b5cec5f495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "answer_col = [col for col in train.columns if col.startswith('answer') and\n",
        "              col not in ['answer_user_name', 'answer_user_page']] + ['qa_id']\n",
        "answer = train[answer_col]\n",
        "\n",
        "answer_chunk = pd.DataFrame()\n",
        "for row in tqdm(range(len(answer))):\n",
        "    if len(answer.loc[row, 'answer']) % 256 != 0:\n",
        "        replicate_row = len(answer.loc[row, 'answer']) // 256 + 1\n",
        "    else:\n",
        "        replicate_row = len(answer.loc[row, 'answer']) // 256\n",
        "    for i in range(replicate_row):\n",
        "        tmp = answer.loc[row].copy()\n",
        "        tmp.loc['answer_chunk'] = tmp['answer'][i*256: (i+1)*256]\n",
        "        answer_chunk = answer_chunk.append(tmp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a40df0bab832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'answer_chunk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHK5JvMJpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 256 #@param {type:\"slider\", min:128, max:512, step:32}\n",
        "BATCH_SIZE = 8 #@param {type:'slider', min:8, max:32, step:8}\n",
        "NUM_EPOCHS = 3\n",
        "NUM_CLASSES = 30\n",
        "LR = 5e-5\n",
        "MIN_LR = 0\n",
        "# OUTPUT_TRAIN = 'train_bert_ipredcv1415_oof.csv'\n",
        "# OUTPUT_TEST = 'test_bert_large.npy'\n",
        "model_path = 'uncased_L-12_H-768_A-12' #@param ['uncased_L-12_H-768_A-12', 'wwm_uncased_L-24_H-1024_A-16', 'uncased_L-24_H-1024_A-16']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2llbNDMhVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(os.path.join(model_path, 'vocab.txt'), 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4zB4cIjMjFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df, old_df, branch='training', type='answer'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    global MAXLEN\n",
        "    q_title, q_body, answer = [], [], [] \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        if type=='answer':\n",
        "            answer = []\n",
        "            answer_ids, _ = tokenizer.encode(data_df.loc[i, 'answer_chunk'][:MAXLEN])\n",
        "            answer.append(answer_ids)\n",
        "        else:                       \n",
        "            q_title_ids, _ = tokenizer.encode(data_df.loc[i, 'question_title'][:MAXLEN])\n",
        "            q_title.append(q_title_ids)\n",
        "            q_body_ids, _ = tokenizer.encode(data_df.loc[i, 'question_body'][:MAXLEN])\n",
        "            q_body.append(q_body_ids)\n",
        "\n",
        "    # aux = data_df[[col for col in data_df.columns if col not in old_df.columns]]\n",
        "    if branch == 'training':\n",
        "        targets = data_df.iloc[:, 11:41]\n",
        "        return [q_title, q_body, answer], np.array(targets)\n",
        "    else:\n",
        "        return [q_title, q_body, answer]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7qwb8ZMlbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Data Generator\"\"\"\n",
        "def seq_padding(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = max(L)\n",
        "    return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n",
        "\n",
        "class data_generator:\n",
        "    def __init__(self, data, batch_size=BATCH_SIZE, branch='train', q_a='a'):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.branch = branch\n",
        "        self.q_a = q_a\n",
        "        self.steps = len(self.data) // self.batch_size\n",
        "        if len(self.data) % self.batch_size != 0:\n",
        "            self.steps += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if self.branch == 'train':\n",
        "                np.random.shuffle(self.data)\n",
        "            for i in range(self.steps):\n",
        "                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "                if q_a == 'q':\n",
        "                    X1 = seq_padding([x[0] for x in d])\n",
        "                    X2 = np.zeros_like(X1)\n",
        "                    X3 = seq_padding([x[1] for x in d])\n",
        "                    X4 = np.zeros_like(X3)\n",
        "                    if self.branch == 'test':\n",
        "                        # aux = np.array([x[3] for x in d])\n",
        "                        yield [X1, X2, X3, X4]\n",
        "                    else:\n",
        "                        Y = np.array([x[2] for x in d])\n",
        "                        # aux = np.array([x[4] for x in d])\n",
        "                        yield [X1, X2, X3, X4], Y\n",
        "                else:\n",
        "                    X1 = seq_padding([x[0] for x in d])\n",
        "                    X2 = np.zeros_like(X1)\n",
        "                    if self.branch == 'test':\n",
        "                        # aux = np.array([x[3] for x in d])\n",
        "                        yield [X1, X2]\n",
        "                    else:\n",
        "                        Y = np.array([x[1] for x in d])\n",
        "                        # aux = np.array([x[4] for x in d])\n",
        "                        yield [X1, X2], Y\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvVEWF62IdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_build(len_train, q_a='a'):\n",
        "    global NUM_CLASSES\n",
        "    global BATCH_SIZE\n",
        "    global NUM_EPOCHS\n",
        "    global MIN_LR\n",
        "    global LR\n",
        "    global MAXLEN\n",
        "\n",
        "    bert_model = load_trained_model_from_checkpoint(\n",
        "        os.path.join(model_path, 'bert_config.json'),\n",
        "        os.path.join(model_path, 'bert_model.ckpt'),\n",
        "        seq_len = MAXLEN,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    if q_a == 'q':\n",
        "        q_in = Input(shape=(None,))\n",
        "        q2_in = Input(shape=(None,))\n",
        "        qb_in = Input(shape=(None,))\n",
        "        qb2_in = Input(shape=(None,))\n",
        "\n",
        "        q_inputs = bert_model([q_in, q2_in])\n",
        "        q_outputs = Lambda(lambda x: x[:, 0])(q_inputs)\n",
        "        qb_inputs = bert_model([qb_in, qb2_in])\n",
        "        qb_outputs = Lambda(lambda x: x[:, 0])(qb_inputs)\n",
        "\n",
        "        dense = concatenate([q_outputs, qb_outputs])\n",
        "        outputs = Dense(NUM_CLASSES, activation='softmax')(dense)\n",
        "        model = Model([q_in, q2_in, qb_in, qb2_in], outputs)\n",
        "    else:\n",
        "        a_in = Input(shape=(None,))\n",
        "        a2_in = Input(shape=(None,))\n",
        "        # aux_in = Input(shape=(aux.shape[1], ))\n",
        "\n",
        "        a_inputs = bert_model([a_in, a2_in])\n",
        "        a_outputs = Lambda(lambda x: x[:, 0])(a_inputs)\n",
        "\n",
        "        dense = a_outputs\n",
        "        outputs = Dense(NUM_CLASSES, activation='softmax')(dense)\n",
        "        model = Model([a_in, a2_in], outputs)\n",
        "\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        # optimizer=Adam(1e-5),\n",
        "        optimizer=AdamWarmup(\n",
        "            decay_steps=decay_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            lr=LR,\n",
        "            min_lr=MIN_LR,\n",
        "            ))\n",
        "    del bert_model\n",
        "    gc.collect()\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH88F-DJMsTR",
        "colab_type": "code",
        "outputId": "d7b8c975-172c-4676-b8c5-018bb68b918e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_x, test_aux = convert_data(test_aux, test, branch='testing')\n",
        "pred = np.zeros((len(test), NUM_CLASSES))\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "idx = [x for x in kf.split(train_aux)]\n",
        "\n",
        "def compute_spearmanr(trues, preds):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
        "    return np.nanmean(rhos)\n",
        "\n",
        "class IntervalEval(Callback):\n",
        "    def __init__(self, test_data, label):\n",
        "        super(Callback, self).__init__()\n",
        "        self.test_data = test_data\n",
        "        self.label = label\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        preds = self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1)\n",
        "        score = compute_spearmanr(self.label, preds)\n",
        "        print('Epoch - {:} - Spearman - {:.5f}'.format(epoch+1, score))\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx[1:], 1):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train_aux.loc[tr_idx], train_aux.loc[val_idx]\n",
        "    tr_x, tr_y, tr_aux = convert_data(tr, train)\n",
        "    val_x, val_y, val_aux = convert_data(val, train)\n",
        "    \n",
        "    model = model_build(len_train=len(tr_aux))\n",
        "\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_x[2], tr_y, tr_aux)))\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_x[2], val_y, val_aux)), branch='valid')\n",
        "    test_D = data_generator(list(zip(test_x[0], test_x[1], test_x[2], test_aux)), branch='test')\n",
        "    ieval = IntervalEval(test_data=valid_D, label=val_y)\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "    # oof_pred = model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    # train_aug.loc[val_idx, 'oof_pred'] = np.argmax(oof_pred, 1)\n",
        "    # print('oof - {:} f1_score - {:.4f}'.format(i, spearmanr(val_y, np.argmax(oof_pred, 1), average='weighted')))\n",
        "\n",
        "    pred+= model.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n",
        "    # np.save(OUTPUT_TEST, pred)\n",
        "    # s3.upload_file(Filename=OUTPUT_TEST, Bucket='acmilannesta', Key='large/'+OUTPUT_TEST)\n",
        "\n",
        "    # model_file = 'model-oof-'+str(i)+'.h5'\n",
        "    # model.save('model.h5')\n",
        "\n",
        "    del model\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 47/4863 [00:00<00:10, 461.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4863/4863 [00:10<00:00, 475.20it/s]\n",
            "100%|██████████| 1216/1216 [00:02<00:00, 467.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "608/608 [==============================] - 315s 518ms/step - loss: 1.4769\n",
            "152/152 [==============================] - 27s 181ms/step\n",
            "Epoch - 1 - Spearman - 0.30811\n",
            "Epoch 2/3\n",
            "608/608 [==============================] - 275s 452ms/step - loss: 1.4476\n",
            "152/152 [==============================] - 19s 126ms/step\n",
            "Epoch - 2 - Spearman - 0.32974\n",
            "Epoch 3/3\n",
            "608/608 [==============================] - 274s 451ms/step - loss: 1.4371\n",
            "152/152 [==============================] - 19s 126ms/step\n",
            "Epoch - 3 - Spearman - 0.33309\n",
            "60/60 [==============================] - 8s 131ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 48/4863 [00:00<00:10, 475.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4863/4863 [00:09<00:00, 495.07it/s]\n",
            "100%|██████████| 1216/1216 [00:02<00:00, 496.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "608/608 [==============================] - 324s 533ms/step - loss: 1.4751\n",
            "152/152 [==============================] - 31s 203ms/step\n",
            "Epoch - 1 - Spearman - 0.30905\n",
            "Epoch 2/3\n",
            "608/608 [==============================] - 275s 452ms/step - loss: 1.4489\n",
            "152/152 [==============================] - 19s 128ms/step\n",
            "Epoch - 2 - Spearman - 0.32739\n",
            "Epoch 3/3\n",
            "608/608 [==============================] - 275s 452ms/step - loss: 1.4383\n",
            "152/152 [==============================] - 20s 128ms/step\n",
            "Epoch - 3 - Spearman - 0.32910\n",
            "60/60 [==============================] - 8s 133ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 52/4863 [00:00<00:09, 516.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 4\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4863/4863 [00:09<00:00, 490.07it/s]\n",
            "100%|██████████| 1216/1216 [00:02<00:00, 496.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "608/608 [==============================] - 333s 548ms/step - loss: 1.4743\n",
            "152/152 [==============================] - 34s 224ms/step\n",
            "Epoch - 1 - Spearman - 0.30311\n",
            "Epoch 2/3\n",
            "608/608 [==============================] - 278s 457ms/step - loss: 1.4467\n",
            "152/152 [==============================] - 19s 127ms/step\n",
            "Epoch - 2 - Spearman - 0.33494\n",
            "Epoch 3/3\n",
            "608/608 [==============================] - 277s 456ms/step - loss: 1.4360\n",
            "152/152 [==============================] - 19s 128ms/step\n",
            "Epoch - 3 - Spearman - 0.33626\n",
            "60/60 [==============================] - 8s 132ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 49/4864 [00:00<00:09, 485.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4864/4864 [00:10<00:00, 482.49it/s]\n",
            "100%|██████████| 1215/1215 [00:02<00:00, 487.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 42/608 [=>............................] - ETA: 18:46 - loss: 1.6493"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0feddfc2f6a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mieval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# oof_pred = model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[96,290,290] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_14_2/Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention-Attention/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss_4/mul/_23179]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[96,290,290] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_14_2/Encoder-11-MultiHeadSelfAttention/Encoder-11-MultiHeadSelfAttention-Attention/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    }
  ]
}