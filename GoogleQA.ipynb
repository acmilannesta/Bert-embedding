{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleQA.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/GoogleQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82bieSpLcuc",
        "colab_type": "code",
        "outputId": "f0e4f857-efc1-4725-9cbf-339bf7dba4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 10 14:44:16 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    35W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErzYVOxLtWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-bert\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBrwK8dwLyw8",
        "colab_type": "code",
        "outputId": "d1c5ae9a-8034-4f58-84c2-dc233e594752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaDm4NWvLp_J",
        "colab_type": "code",
        "outputId": "ecdc2ac2-26f6-4f25-842b-7445b8a5a421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, gc\n",
        "import codecs\n",
        "from keras.layers import *\n",
        "from keras.callbacks import Callback\n",
        "from keras.models import Model, load_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps, get_custom_objects\n",
        "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xGojs4dL6S4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'drive/My Drive/GoogleQA/Data/'\n",
        "train = pd.read_csv(data_path+'train.csv')\n",
        "test = pd.read_csv(data_path+'test.csv')\n",
        "sub = pd.read_csv(data_path+'sample_submission.csv')\n",
        "\n",
        "# train_q = pd.read_csv(data_path+'train_q.csv')\n",
        "# test_q = pd.read_csv(data_path+'test_q.csv')\n",
        "# train_a = pd.read_csv(data_path+'train_answer.csv')\n",
        "# test_a = pd.read_csv(data_path+'test_answer.csv')\n",
        "\n",
        "# df_aux = pd.concat([train[['host', 'category']], test[['host', 'category']]], 0)\n",
        "# aux1 = pd.get_dummies(df_aux['category'], drop_first=True)\n",
        "# aux2 = pd.get_dummies(df_aux['host'].apply(lambda x: x.split('.')[0]), drop_first=True)\n",
        "# aux = pd.concat([aux1, aux2], 1)\n",
        "# train_aux = pd.concat([train, aux.iloc[:len(train), ]], 1)\n",
        "# test_aux =  pd.concat([test, aux.iloc[len(train):, ]], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAkcSF4CBvZN",
        "colab_type": "code",
        "outputId": "669f5031-bc45-430a-82a8-d103ad41acb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "test.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
              "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
              "       'url', 'category', 'host'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH_AbgWGwUIi",
        "colab_type": "text"
      },
      "source": [
        "##Chunknize answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYMg8WaswSx0",
        "colab_type": "code",
        "outputId": "fc3e147b-1c0c-43e5-fe17-5949b440175b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# def answer_chunk(df):\n",
        "#     answer_col = ['qa_id'] + [col for col in df.columns if col.startswith('answer') and\n",
        "#                 col not in ['answer_user_name', 'answer_user_page']] \n",
        "#     answer = df[answer_col]\n",
        "\n",
        "#     answer_chunk = pd.DataFrame()\n",
        "#     for row in tqdm(range(len(answer))):\n",
        "#         if len(answer.loc[row, 'answer']) % 256 != 0:\n",
        "#             replicate_row = len(answer.loc[row, 'answer']) // 256 + 1\n",
        "#         else:\n",
        "#             replicate_row = len(answer.loc[row, 'answer']) // 256\n",
        "#         for i in range(replicate_row):\n",
        "#             tmp = answer.loc[row].copy()\n",
        "#             tmp.loc['answer_chunk'] = tmp['answer'][i*256: (i+1)*256]\n",
        "#             answer_chunk = answer_chunk.append(tmp, ignore_index=True)\n",
        "#     return answer_chunk\n",
        "\n",
        "# def question_chunk(df):\n",
        "#     q_col = ['qa_id'] + [col for col in df.columns if col.startswith('question') and\n",
        "#                 col not in ['question_user_name', 'question_user_name']] \n",
        "#     q = df[q_col]\n",
        "\n",
        "#     q_chunk = pd.DataFrame()\n",
        "#     for row in tqdm(range(len(q))):\n",
        "#         if len(q.loc[row, 'question_body']) % 256 != 0:\n",
        "#             replicate_row = len(q.loc[row, 'question_body']) // 256 + 1\n",
        "#         else:\n",
        "#             replicate_row = len(q.loc[row, 'question_body']) // 256\n",
        "#         for i in range(replicate_row):\n",
        "#             tmp = q.loc[row].copy()\n",
        "#             tmp.loc['qbody_chunk'] = tmp['question_body'][i*256: (i+1)*256]\n",
        "#             q_chunk = q_chunk.append(tmp, ignore_index=True)\n",
        "#     return q_chunk\n",
        "\n",
        "# train_q, test_q = question_chunk(train), question_chunk(test)\n",
        "# train_a, test_a = answer_chunk(train), answer_chunk(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 476/476 [00:09<00:00, 52.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHK5JvMJpY",
        "colab_type": "code",
        "outputId": "c405ac9d-8441-4dbe-e207-5f232be425ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "MAXLEN = 512 #@param {type:\"slider\", min:128, max:512, step:32}\n",
        "BATCH_SIZE = 4 #@param {type:'slider', min:4, max:32, step:4}\n",
        "NUM_EPOCHS = 3\n",
        "NUM_CLASSES = 30\n",
        "LR = 5e-5\n",
        "MIN_LR = 0\n",
        "# OUTPUT_TRAIN = 'train_bert_ipredcv1415_oof.csv'\n",
        "# OUTPUT_TEST = 'test_bert_large.npy'\n",
        "model_path = 'uncased_L-12_H-768_A-12' #@param ['uncased_L-12_H-768_A-12', 'wwm_uncased_L-24_H-1024_A-16', 'uncased_L-24_H-1024_A-16']\n",
        "# target_q_col = train.columns.tolist()[11:32]\n",
        "# target_a_col = train.columns.tolist()[32:42]\n",
        "target_col = train.columns.tolist()[11:42]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['question_asker_intent_understanding',\n",
              " 'question_body_critical',\n",
              " 'question_conversational',\n",
              " 'question_expect_short_answer',\n",
              " 'question_fact_seeking',\n",
              " 'question_has_commonly_accepted_answer',\n",
              " 'question_interestingness_others',\n",
              " 'question_interestingness_self',\n",
              " 'question_multi_intent',\n",
              " 'question_not_really_a_question',\n",
              " 'question_opinion_seeking',\n",
              " 'question_type_choice',\n",
              " 'question_type_compare',\n",
              " 'question_type_consequence',\n",
              " 'question_type_definition',\n",
              " 'question_type_entity',\n",
              " 'question_type_instructions',\n",
              " 'question_type_procedure',\n",
              " 'question_type_reason_explanation',\n",
              " 'question_type_spelling',\n",
              " 'question_well_written',\n",
              " 'answer_helpful',\n",
              " 'answer_level_of_information',\n",
              " 'answer_plausible',\n",
              " 'answer_relevance',\n",
              " 'answer_satisfaction',\n",
              " 'answer_type_instructions',\n",
              " 'answer_type_procedure',\n",
              " 'answer_type_reason_explanation',\n",
              " 'answer_well_written']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2llbNDMhVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(os.path.join(model_path, 'vocab.txt'), 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4zB4cIjMjFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df, branch='training'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    global MAXLEN\n",
        "    global target_col\n",
        "    # global target_a_col\n",
        "    # global target_q_col\n",
        "    q_title, q_body, answer = [], [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        answer_ids, _ = tokenizer.encode(data_df.loc[i, 'answer'][:MAXLEN])\n",
        "        answer.append(answer_ids)                 \n",
        "        q_title_ids, _ = tokenizer.encode(data_df.loc[i, 'question_title'][:MAXLEN])\n",
        "        q_title.append(q_title_ids)\n",
        "        q_body_ids, _ = tokenizer.encode(data_df.loc[i, 'question_body'][:MAXLEN])\n",
        "        q_body.append(q_body_ids)\n",
        "    if branch == 'training':\n",
        "        targets = data_df[target_col]\n",
        "        return [q_title, q_body, answer], np.array(targets)\n",
        "    else:\n",
        "        return [q_title, q_body, answer]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7qwb8ZMlbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Data Generator\"\"\"\n",
        "def seq_padding(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = max(L)\n",
        "    return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n",
        "\n",
        "class data_generator:\n",
        "    def __init__(self, data, batch_size=BATCH_SIZE, branch='train'):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.branch = branch\n",
        "        # self.q_a = q_a\n",
        "        self.steps = len(self.data) // self.batch_size\n",
        "        if len(self.data) % self.batch_size != 0:\n",
        "            self.steps += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if self.branch == 'train':\n",
        "                np.random.shuffle(self.data)\n",
        "            for i in range(self.steps):\n",
        "                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "                X1 = seq_padding([x[0] for x in d])\n",
        "                X2 = np.zeros_like(X1)\n",
        "                X3 = seq_padding([x[1] for x in d])\n",
        "                X4 = np.zeros_like(X3)\n",
        "                X5 = seq_padding([x[2] for x in d])\n",
        "                X6 = np.zeros_like(X5)\n",
        "                if self.branch == 'test':\n",
        "                    # aux = np.array([x[3] for x in d])\n",
        "                    yield [X1, X2, X3, X4, X5, X6]\n",
        "                else:\n",
        "                    Y = np.array([x[3] for x in d])\n",
        "                    # aux = np.array([x[4] for x in d])\n",
        "                    yield [X1, X2, X3, X4, X5, X6], Y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvVEWF62IdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_build(len_train):\n",
        "    global NUM_CLASSES\n",
        "    global BATCH_SIZE\n",
        "    global NUM_EPOCHS\n",
        "    global MIN_LR\n",
        "    global LR\n",
        "    global MAXLEN\n",
        "\n",
        "    bert_model = load_trained_model_from_checkpoint(\n",
        "        os.path.join(model_path, 'bert_config.json'),\n",
        "        os.path.join(model_path, 'bert_model.ckpt'),\n",
        "        seq_len = MAXLEN,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    q_in = Input(shape=(None,))\n",
        "    q2_in = Input(shape=(None,))\n",
        "    qb_in = Input(shape=(None,))\n",
        "    qb2_in = Input(shape=(None,))\n",
        "\n",
        "    a_in = Input(shape=(None,))\n",
        "    a2_in = Input(shape=(None,))\n",
        "    # aux_in = Input(shape=(aux.shape[1], ))\n",
        "\n",
        "    a_inputs = bert_model([a_in, a2_in])\n",
        "    a_outputs = Lambda(lambda x: x[:, 0])(a_inputs)\n",
        "\n",
        "    q_inputs = bert_model([q_in, q2_in])\n",
        "    q_outputs = Lambda(lambda x: x[:, 0])(q_inputs)\n",
        "    qb_inputs = bert_model([qb_in, qb2_in])\n",
        "    qb_outputs = Lambda(lambda x: x[:, 0])(qb_inputs)\n",
        "\n",
        "    dense = concatenate([q_outputs, qb_outputs, a_outputs])\n",
        "    outputs = Dense(NUM_CLASSES, activation='softmax')(dense)\n",
        "    model = Model([q_in, q2_in, qb_in, qb2_in, a_in, a2_in], outputs)\n",
        "\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        # optimizer=Adam(1e-5),\n",
        "        optimizer=AdamWarmup(\n",
        "            decay_steps=decay_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            lr=LR,\n",
        "            min_lr=MIN_LR,\n",
        "            ))\n",
        "    del bert_model\n",
        "    gc.collect()\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N56RysQWJ6R1",
        "colab_type": "text"
      },
      "source": [
        "## train on answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH88F-DJMsTR",
        "colab_type": "code",
        "outputId": "4abb690c-4623-4b92-fd59-bb2d39af2b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_x = convert_data(test, branch='testing')\n",
        "pred = np.zeros((len(test), NUM_CLASSES))\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "idx = [x for x in kf.split(train)]\n",
        "\n",
        "def compute_spearmanr(trues, preds):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
        "    return np.nanmean(rhos)\n",
        "\n",
        "class IntervalEval(Callback):\n",
        "    def __init__(self, test_data, label):\n",
        "        super(Callback, self).__init__()\n",
        "        self.test_data = test_data\n",
        "        self.label = label\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # if epoch == 3:\n",
        "        preds = self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1)\n",
        "        score = compute_spearmanr(self.label, preds)\n",
        "        print('Spearman - {:.5f}'.format(score))\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx[0:1], 1):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    tr_x, tr_y = convert_data(tr)\n",
        "    val_x, val_y = convert_data(val)\n",
        "    \n",
        "    model = model_build(len_train=len(tr))\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_x[2], tr_y)))\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_x[2], val_y)), branch='valid')\n",
        "    test_D = data_generator(list(zip(test_x[0], test_x[1], test_x[2])), branch='test')\n",
        "    ieval = IntervalEval(test_data=valid_D, label=val_y)\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "    # oof_pred = model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    # train_aug.loc[val_idx, 'oof_pred'] = np.argmax(oof_pred, 1)\n",
        "    # print('oof - {:} f1_score - {:.4f}'.format(i, spearmanr(val_y, np.argmax(oof_pred, 1), average='weighted')))\n",
        "\n",
        "    pred+= model.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n",
        "    # np.save(OUTPUT_TEST, pred)\n",
        "    # s3.upload_file(Filename=OUTPUT_TEST, Bucket='acmilannesta', Key='large/'+OUTPUT_TEST)\n",
        "\n",
        "    # model_file = 'model-oof-'+str(i)+'.h5'\n",
        "    # model.save('model.h5')\n",
        "\n",
        "    del model\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az1W9iNAL3_B",
        "colab_type": "code",
        "outputId": "11730b4c-e131-4b6d-dded-993942a35cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}