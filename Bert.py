# -*- coding: utf-8 -*-
"""欢迎使用 Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/welcome.ipynb
"""

from google.colab import files
files.upload()
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification

!git clone https://github.com/google-research/bert.git

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip the file
!unzip uncased_L-12_H-768_A-12.zip

import pandas as pd
from sklearn.model_selection import train_test_split
from pandas import DataFrame
import numpy as np 
train = pd.read_csv("train.csv.zip")
test = pd.read_csv("test.csv.zip")

df_bert = pd.DataFrame({'user_id':train['id'],
            'label':np.where(train.target > .5,1,0),
            'alpha':['a']*train.shape[0],
            'text':train["comment_text"].replace(r'\n',' ',regex=True)})

df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)

# Creating test dataframe according to BERT
df_bert_test = pd.DataFrame({'User_ID':test['id'],
                 'text':test['comment_text'].replace(r'\n',' ',regex=True)})

df_bert_train.to_csv('data/train.tsv', sep='\t', index=False, header=False)
df_bert_dev.to_csv('data/dev.tsv', sep='\t', index=False, header=False)
df_bert_test.to_csv('data/test.tsv', sep='\t', index=False, header=True)

!python bert/run_classifier.py \
--task_name=cola \
--do_train=true \
--do_eval=true \
--do_predict=true \
--data_dir=data \
--vocab_file=uncased_L-12_H-768_A-12/vocab.txt \
--bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \
--init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \
--max_seq_length=400 \
--train_batch_size=32 \
--learning_rate=2e-5 \
--num_train_epochs=1.0 \
--output_dir=bert_output \
--save_checkpoints_steps=9999999

"""BERT feature extraction"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np 
import gc
from tqdm import tqdm
train = pd.read_csv("train.csv.zip")['comment_text'].replace(r'\n', ' ', regex=True)
batch_size = 512
n_batch = len(train) // batch_size
gc.collect()

for batch in tqdm(range(n_batch)):
    batch_start = batch_size * batch
    batch_end = batch_size * (batch+1)
    train[batch_start:batch_end].to_csv('input.txt', header=False, index=False)
    tmp_matrix = np.zeros((batch_size, 768), dtype='float16')
    
    !python bert/extract_features.py \
    --input_file=input.txt \
    --output_file=output.jsonl \
    --vocab_file=uncased_L-12_H-768_A-12/vocab.txt \
    --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json \
    --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt \
    --layers=-1 \
    --max_seq_length=220 \
    --batch_size=64
    
    bert_output = pd.read_json("output.jsonl", lines=True)

    for i in range(batch_size):
        tmp = np.zeros(768, )
        voc = len(bert_output['features'][i])-2
        for j in range(1, voc+1):
            tmp += np.asarray(bert_output['features'][i][j]['layers'][0]['values'], dtype='float16') / voc
        tmp_matrix[i] = tmp
    if batch > 0:
        tmp_matrix = np.vstack((tmp_matrix, np.load('drive/My Drive/Data/bert_matrix.npy')))
    np.save('drive/My Drive/Data/bert_matrix', tmp_matrix)
    del tmp_matrix, bert_output
    gc.collect()

# from bert_embedding import BertEmbedding
# import mxnet as mx
# ctx = mx.gpu(0)
bert = BertEmbedding(batch_size=64)

for batch in tqdm(range(n_batch)):
    batch_start = batch_size * batch
    batch_end = batch_size * (batch+1)
    batch_file = train[batch_start:batch_end].tolist()
    tmp_matrix = np.zeros((batch_size, 768), dtype='float16')
    for i, matrix in enumerate(bert(batch_file)):
      tmp_matrix[i] = np.mean(np.asarray(matrix[1]), 0)
    if batch > 0:
      tmp_matrix = np.vstack((tmp_matrix, np.load('drive/My Drive/Data/bert_matrix.npy')))
    np.save('drive/My Drive/Data/bert_matrix', tmp_matrix)
    del tmp_matrix, matrix, batch_file
    gc.collect()