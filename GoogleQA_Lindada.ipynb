{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_bert_v3.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/GoogleQA_Lindada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iftERautCezS",
        "colab_type": "code",
        "outputId": "ab8f8ac5-bdd3-4179-c4dc-ab3bc082ce41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.1.0-rc0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.1.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/24/f94a1b8f779471f53b814a96c5109994ccf65b0103b771ff208c8a937d37/tensorflow_gpu-2.1.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (402.3MB)\n",
            "\u001b[K     |████████████████████████████████| 402.3MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.17.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (3.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (0.33.6)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 62.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.1.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu==2.1.0-rc0) (42.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (0.16.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.1.0-rc0) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.10.0 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.1.0rc0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2_1_uB0Cpzm",
        "colab_type": "code",
        "outputId": "87aec326-5f22-477a-c300-07e38ddc797f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 25 02:41:06 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_A628bVCp2v",
        "colab_type": "code",
        "outputId": "ec9cde4f-1dec-4302-f0d8-78f7b9d78987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhvY9RBrlTaW",
        "colab_type": "code",
        "outputId": "a5f50f70-bd67-4692-ed32-bfffd19af4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "!top"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[mtop - 03:42:46 up  1:07,  0 users,  load average: 0.16, 0.42, 0.46\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "Tasks:\u001b[m\u001b[m\u001b[1m  14 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m  12 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  9.2 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  4.6 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 85.1 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  1.1 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 26753328 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  4413976 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  5600876 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m 16738476 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 23877696 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\u001b[7m    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND   \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m      1 root      20   0   39196   6372   4848 S   0.0  0.0   0:00.04 run.sh    \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m      8 root      20   0  706192  71420  24976 S   0.0  0.3   0:08.81 node      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m     24 root      20   0  488056 109556  26348 S   0.0  0.4   0:10.23 jupyter-+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    138 root      20   0   35888   4892   3796 S   0.0  0.0   0:00.13 tail      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    247 root      20   0   18376   1492   1192 S   0.0  0.0   0:00.00 bash      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    248 root      20   0 1123684  15584  13676 S   0.0  0.1   0:00.01 drive     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    249 root      20   0   11596   2124   1848 S   0.0  0.0   0:00.00 grep      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    383 root      20   0 1421132 138628  25640 S   0.0  0.5   0:46.14 drive     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    394 root      20   0       0      0      0 Z   0.0  0.0   0:00.49 fusermou+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    427 root      20   0   18376   2968   2720 S   0.0  0.0   0:00.00 bash      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    428 root      20   0    4568    764    700 S   0.0  0.0   0:00.08 tail      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    429 root      20   0   11464   1056    964 S   0.0  0.0   0:00.00 grep      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m   3454 root      20   0 35.439g 5.122g 615676 S   0.0 20.1   3:12.73 python3   \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m\u001b[1m   4385 root      20   0   63176   6732   5008 R   0.0  0.0   0:00.06 top       \u001b[m\u001b[m\u001b[K\n",
            "\u001b[J\u001b[H\u001b[mtop - 03:42:49 up  1:07,  0 users,  load average: 0.14, 0.41, 0.46\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "Tasks:\u001b[m\u001b[m\u001b[1m  14 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mrunning,\u001b[m\u001b[m\u001b[1m  12 \u001b[m\u001b[msleeping,\u001b[m\u001b[m\u001b[1m   0 \u001b[m\u001b[mstopped,\u001b[m\u001b[m\u001b[1m   1 \u001b[m\u001b[mzombie\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "%Cpu(s):\u001b[m\u001b[m\u001b[1m  0.5 \u001b[m\u001b[mus,\u001b[m\u001b[m\u001b[1m  0.4 \u001b[m\u001b[msy,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mni,\u001b[m\u001b[m\u001b[1m 99.1 \u001b[m\u001b[mid,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mwa,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mhi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[msi,\u001b[m\u001b[m\u001b[1m  0.0 \u001b[m\u001b[mst\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Mem :\u001b[m\u001b[m\u001b[1m 26753328 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m  4413296 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m  5601536 \u001b[m\u001b[mused,\u001b[m\u001b[m\u001b[1m 16738496 \u001b[m\u001b[mbuff/cache\u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "KiB Swap:\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mtotal,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mfree,\u001b[m\u001b[m\u001b[1m        0 \u001b[m\u001b[mused.\u001b[m\u001b[m\u001b[1m 23877076 \u001b[m\u001b[mavail Mem \u001b[m\u001b[m\u001b[m\u001b[m\u001b[K\n",
            "\u001b[K\n",
            "\n",
            "\u001b[m   3454 root      20   0 35.439g 5.122g 615676 S   1.0 20.1   3:12.76 python3   \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m     24 root      20   0  488056 109556  26348 S   0.3  0.4   0:10.24 jupyter-+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m      1 root      20   0   39196   6372   4848 S   0.0  0.0   0:00.04 run.sh    \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m      8 root      20   0  706192  71684  24976 S   0.0  0.3   0:08.81 node      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    138 root      20   0   35888   4892   3796 S   0.0  0.0   0:00.13 tail      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    247 root      20   0   18376   1492   1192 S   0.0  0.0   0:00.00 bash      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    248 root      20   0 1123684  15584  13676 S   0.0  0.1   0:00.01 drive     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    249 root      20   0   11596   2124   1848 S   0.0  0.0   0:00.00 grep      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    383 root      20   0 1421132 138628  25640 S   0.0  0.5   0:46.14 drive     \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    394 root      20   0       0      0      0 Z   0.0  0.0   0:00.49 fusermou+ \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    427 root      20   0   18376   2968   2720 S   0.0  0.0   0:00.00 bash      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    428 root      20   0    4568    764    700 S   0.0  0.0   0:00.08 tail      \u001b[m\u001b[m\u001b[K\n",
            "\u001b[m    429 root      20   0   11464   1056    964 S   0.0  0.0   0:00.00 grep      \u001b[m\u001b[m\u001b[K\n",
            "\n",
            "\u001b[J\u001b[?1l\u001b>\u001b[25;1H\n",
            "\u001b[K"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Of-F3-2lYX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 3454"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JOCd5sBCp50",
        "colab_type": "code",
        "outputId": "1174a4e9-bafc-4fc0-9a86-5dddbace0adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHQFaPvRCp8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/GoogleQA/code/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XO0L2-5CqCW",
        "colab_type": "code",
        "outputId": "f3b40609-67f3-4c99-ac2f-299bf90bb0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_lstm.ipynb  bert_weights_RepeatKfold  tf_bert.ipynb     tf_bert_v3.ipynb\n",
            "bert_v4.ipynb\t lstm_use.ipynb\t\t   tf_bert_v2.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RigJA-fkCqFj",
        "colab_type": "code",
        "outputId": "3a38c0f7-4424-4cfd-f0b3-bbdb80c3c5ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold, RepeatedKFold\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import gc\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import six\n",
        "import collections\n",
        "from scipy.stats import spearmanr, rankdata\n",
        "from math import floor, ceil\n",
        "\n",
        "sys.path.extend(['../input/bert-joint-baseline/'])\n",
        "# from transformers import *\n",
        "import tokenization\n",
        "DEBUG = False\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# print('tokenizer: ', tokenizer)\n",
        "\n",
        "if not DEBUG:\n",
        "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n",
        "    np.set_printoptions(suppress=True)\n",
        "\n",
        "PATH = '../input/google-quest-challenge/'\n",
        "BERT_PATH = '../input/bert-en-uncased-l12-h768-a12'\n",
        "# BERT_PATH = '../input/bertenuncasedl24h1024a16'\n",
        "tokenizer = tokenization.FullTokenizer(BERT_PATH+'/assets/vocab.txt', False)\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "\n",
        "if DEBUG:\n",
        "    df_train = pd.read_csv(PATH+'train.csv', nrows=30)\n",
        "    df_test = pd.read_csv(PATH+'test.csv', nrows=30)\n",
        "else:\n",
        "    df_train = pd.read_csv(PATH + 'train.csv')\n",
        "    df_test = pd.read_csv(PATH + 'test.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)\n",
        "\n",
        "output_categories = list(df_train.columns[11:])\n",
        "print('\\noutput categories:\\n\\t', output_categories)\n",
        "\n",
        "targets = [\n",
        "        'question_asker_intent_understanding',\n",
        "        'question_body_critical',\n",
        "        'question_conversational',\n",
        "        'question_expect_short_answer',\n",
        "        'question_fact_seeking',\n",
        "        'question_has_commonly_accepted_answer',\n",
        "        'question_interestingness_others',\n",
        "        'question_interestingness_self',\n",
        "        'question_multi_intent',\n",
        "        'question_not_really_a_question',\n",
        "        'question_opinion_seeking',\n",
        "        'question_type_choice',\n",
        "        'question_type_compare',\n",
        "        'question_type_consequence',\n",
        "        'question_type_definition',\n",
        "        'question_type_entity',\n",
        "        'question_type_instructions',\n",
        "        'question_type_procedure',\n",
        "        'question_type_reason_explanation',\n",
        "        'question_type_spelling',\n",
        "        'question_well_written',\n",
        "        'answer_helpful',\n",
        "        'answer_level_of_information',\n",
        "        'answer_plausible',\n",
        "        'answer_relevance',\n",
        "        'answer_satisfaction',\n",
        "        'answer_type_instructions',\n",
        "        'answer_type_procedure',\n",
        "        'answer_type_reason_explanation',\n",
        "        'answer_well_written'\n",
        "    ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape = (6079, 41)\n",
            "test shape = (476, 11)\n",
            "\n",
            "output categories:\n",
            "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ_3gRGtCqLK",
        "colab_type": "code",
        "outputId": "cfa72dc0-19c8-4554-a72c-15e015cabb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1] * len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens) > max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def my_pad(text, max_length, tokenizer):\n",
        "    res = tokenizer.tokenize(text)\n",
        "    if len(res) > max_length:\n",
        "        head_length = int(0.25 * max_length)\n",
        "        tail_length = max_length - head_length\n",
        "        res = res[:head_length] + res[-tail_length:]\n",
        "    return res\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length,\n",
        "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "\n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len + q_len + a_len + 4) > max_sequence_length:\n",
        "\n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len) / 2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len) / 2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "\n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len\n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "\n",
        "        if t_new_len + a_new_len + q_new_len + 4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"\n",
        "                             % (max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n",
        "\n",
        "        head_t_new_len = int(0.25 * t_new_len)\n",
        "        tail_t_new_len = t_new_len - head_t_new_len\n",
        "\n",
        "        head_q_new_len = int(0.25 * q_new_len)\n",
        "        tail_q_new_len = q_new_len - head_q_new_len\n",
        "\n",
        "        head_a_new_len = int(0.25 * a_new_len)\n",
        "        tail_a_new_len = a_new_len - head_a_new_len\n",
        "\n",
        "        t = t[:head_t_new_len] + t[-tail_t_new_len:]\n",
        "        q = q[:head_q_new_len] + q[-tail_q_new_len:]\n",
        "        a = a[:head_a_new_len] + a[-tail_a_new_len:]\n",
        "\n",
        "    return t, q, a\n",
        "\n",
        "def prepare_data(df, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in tqdm(df[['question_title', 'question_body', 'answer', 'question_user_name', 'answer_user_name']].iterrows()):\n",
        "        question_title_ = str(instance['question_title']).lower()\n",
        "        question_body_ = str(instance['question_body']).lower()\n",
        "        answer_ = str(instance['answer']).lower()\n",
        "#         question_user_name_ = instance['question_user_name']\n",
        "#         answer_user_name_ = instance['answer_user_name']\n",
        "\n",
        "#         question_title_ = my_pad(question_title_, max_length=15, tokenizer=tokenizer)\n",
        "#         question_body_ = my_pad(question_body_, max_length=245, tokenizer=tokenizer)\n",
        "#         answer_ = my_pad(answer_, max_length=248, tokenizer=tokenizer)\n",
        "#         question_user_name_ = my_pad(question_user_name_, max_length=2, tokenizer=tokenizer)\n",
        "#         answer_user_name_ = my_pad(answer_user_name_, max_length=2, tokenizer=tokenizer)\n",
        "\n",
        "        question_title_, question_body_, answer_ = _trim_input(question_title_, question_body_, answer_, MAX_SEQUENCE_LENGTH,\n",
        "                                                               t_max_len=15, q_max_len=245, a_max_len=248)\n",
        "\n",
        "        stoken = [\"[CLS]\"] + question_title_ + [\",\"] + question_body_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"]\n",
        "\n",
        "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
        "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "        input_ids.append(input_ids_)\n",
        "        input_masks.append(input_masks_)\n",
        "        input_segments.append(input_segments_)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32),\n",
        "            np.asarray(input_masks, dtype=np.int32),\n",
        "            np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])\n",
        "\n",
        "outputs = compute_output_arrays(df_train, output_categories)\n",
        "\n",
        "inputs = prepare_data(df_train, tokenizer)\n",
        "test_inputs = prepare_data(df_test, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6079it [00:24, 249.87it/s]\n",
            "476it [00:01, 242.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ2iONaJCqOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyLabelEncoder(object):\n",
        "    \"\"\"safely handle unknown label\"\"\"\n",
        "    def __init__(self):\n",
        "        self.mapper = {}\n",
        "\n",
        "    def fit(self, X):\n",
        "        uniq_X = np.unique(X)\n",
        "        # reserve 0 for unknown\n",
        "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "    def _map(self, x):\n",
        "        return self.mapper.get(x, 0)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return list(map(self._map, X))\n",
        "def get_cate_feat(df, isTrain):\n",
        "    if isTrain:\n",
        "        label_encoder[\"category\"] = MyLabelEncoder()\n",
        "        category_feat = np.array(label_encoder[\"category\"].fit_transform(df[\"category\"]))\n",
        "        label_encoder[\"host\"] = MyLabelEncoder()\n",
        "        host_feat = np.array(label_encoder[\"host\"].fit_transform(df[\"host\"]))\n",
        "    else:\n",
        "        category_feat = np.array(label_encoder[\"category\"].transform(df[\"category\"]))\n",
        "        host_feat = np.array(label_encoder[\"host\"].transform(df[\"host\"]))\n",
        "    return [category_feat, host_feat]\n",
        "label_encoder = {}\n",
        "cate_feat_tr = get_cate_feat(df_train, isTrain=True)\n",
        "cate_feat_te = get_cate_feat(df_test, isTrain=False)\n",
        "\n",
        "inputs.extend(cate_feat_tr)\n",
        "test_inputs.extend(cate_feat_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osWNdtLY09GL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_encoder = {}\n",
        "label_encoder[\"category\"] = MyLabelEncoder()\n",
        "label_encoder[\"category\"].fit(df_train[\"category\"])\n",
        "label_encoder[\"host\"] = MyLabelEncoder()\n",
        "label_encoder[\"host\"].fit(df_test[\"host\"])\n",
        "\n",
        "def data_generator(data, batch_size, shuffle=False, isTrain=True):\n",
        "  idx = np.arange(len(data))\n",
        "  if shuffle:\n",
        "    np.random.shuffle(idx)\n",
        "  batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)/batch_size+1)]\n",
        "  while True:\n",
        "    for i in batches:\n",
        "      data_slice = data.iloc[i]\n",
        "      if isTrain:\n",
        "        y = data_slice[targets].values\n",
        "\n",
        "      category_feat = np.array(label_encoder[\"category\"].transform(data_slice[\"category\"]))\n",
        "      host_feat = np.array(label_encoder[\"host\"].transform(data_slice[\"host\"]))\n",
        "\n",
        "      input_ids, input_masks, input_segments = [], [], []\n",
        "      for _, instance in tqdm(data_slice[['question_title', 'question_body', 'answer']].iterrows()):\n",
        "        question_title_ = str(instance['question_title']).lower()\n",
        "        question_body_ = str(instance['question_body']).lower()\n",
        "        answer_ = str(instance['answer']).lower()\n",
        "        \n",
        "        question_title_, question_body_, answer_ = _trim_input(question_title_, question_body_, answer_, MAX_SEQUENCE_LENGTH,\n",
        "                                                                t_max_len=15, q_max_len=245, a_max_len=248)\n",
        "        \n",
        "        stoken = [\"[CLS]\"] + question_title_ + [\",\"] + question_body_ + [\"[SEP]\"] + answer_ + [\"[SEP]\"]\n",
        "        \n",
        "        input_ids_ = _get_ids(stoken, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "        input_masks_ = _get_masks(stoken, MAX_SEQUENCE_LENGTH)\n",
        "        input_segments_ = _get_segments(stoken, MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        input_ids.append(input_ids_)\n",
        "        input_masks.append(input_masks_)\n",
        "        input_segments.append(input_segments_)\n",
        "      X = [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32),\n",
        "           category_feat, host_feat]\n",
        "\n",
        "      if isTrain:\n",
        "        yield (X, y)\n",
        "      else:\n",
        "        yield X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFwhE8BMCqRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
        "class CyclicLR(tf.keras.callbacks.Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or\n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "\n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "\n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore\n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where\n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored\n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on\n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** (x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
        "                self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_K-Acv7-0mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
        "from tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n",
        "from tensorflow.python.keras import backend_config\n",
        "\n",
        "__all__ = ['AdamWarmup']\n",
        "\n",
        "\n",
        "class AdamWarmup(OptimizerV2):\n",
        "    \"\"\"Adam optimizer with warmup.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 decay_steps,\n",
        "                 warmup_steps,\n",
        "                 min_lr=0.0,\n",
        "                 learning_rate=0.001,\n",
        "                 beta_1=0.9,\n",
        "                 beta_2=0.999,\n",
        "                 epsilon=1e-7,\n",
        "                 weight_decay=0.,\n",
        "                 weight_decay_pattern=None,\n",
        "                 amsgrad=False,\n",
        "                 name='Adam',\n",
        "                 **kwargs):\n",
        "        r\"\"\"Construct a new Adam optimizer.\n",
        "\n",
        "        Args:\n",
        "            decay_steps: Learning rate will decay linearly to zero in decay steps.\n",
        "            warmup_steps: Learning rate will increase linearly to lr in first warmup steps.\n",
        "            lr: float >= 0. Learning rate.\n",
        "            beta_1: float, 0 < beta < 1. Generally close to 1.\n",
        "            beta_2: float, 0 < beta < 1. Generally close to 1.\n",
        "            epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
        "            weight_decay: float >= 0. Weight decay.\n",
        "            weight_decay_pattern: A list of strings. The substring of weight names to be decayed.\n",
        "                                  All weights will be decayed if it is None.\n",
        "            amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
        "                algorithm from the paper \"On the Convergence of Adam and\n",
        "                Beyond\".\n",
        "        \"\"\"\n",
        "\n",
        "        super(AdamWarmup, self).__init__(name, **kwargs)\n",
        "        self._set_hyper('decay_steps', float(decay_steps))\n",
        "        self._set_hyper('warmup_steps', float(warmup_steps))\n",
        "        self._set_hyper('min_lr', min_lr)\n",
        "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self._set_hyper('beta_1', beta_1)\n",
        "        self._set_hyper('beta_2', beta_2)\n",
        "        self._set_hyper('weight_decay', weight_decay)\n",
        "        self.epsilon = epsilon or backend_config.epsilon()\n",
        "        self.amsgrad = amsgrad\n",
        "        self._initial_weight_decay = weight_decay\n",
        "        self._weight_decay_pattern = weight_decay_pattern\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'm')\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, 'v')\n",
        "        if self.amsgrad:\n",
        "            for var in var_list:\n",
        "                self.add_slot(var, 'vhat')\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        if len(weights) == 3 * num_vars + 1:\n",
        "            weights = weights[:len(params)]\n",
        "        super(AdamWarmup, self).set_weights(weights)\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)\n",
        "        m = self.get_slot(var, 'm')\n",
        "        v = self.get_slot(var, 'v')\n",
        "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
        "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
        "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "\n",
        "        decay_steps = self._get_hyper('decay_steps', var_dtype)\n",
        "        warmup_steps = self._get_hyper('warmup_steps', var_dtype)\n",
        "        min_lr = self._get_hyper('min_lr', var_dtype)\n",
        "        lr_t = tf.where(\n",
        "            local_step <= warmup_steps,\n",
        "            lr_t * (local_step / warmup_steps),\n",
        "            min_lr + (lr_t - min_lr) * (1.0 - tf.minimum(local_step, decay_steps) / decay_steps),\n",
        "        )\n",
        "        lr_t = (lr_t * math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n",
        "\n",
        "        m_t = state_ops.assign(m,\n",
        "                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n",
        "                               use_locking=self._use_locking)\n",
        "\n",
        "        v_t = state_ops.assign(v,\n",
        "                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n",
        "                               use_locking=self._use_locking)\n",
        "\n",
        "        if self.amsgrad:\n",
        "            v_hat = self.get_slot(var, 'vhat')\n",
        "            v_hat_t = math_ops.maximum(v_hat, v_t)\n",
        "            var_update = m_t / (math_ops.sqrt(v_hat_t) + epsilon_t)\n",
        "        else:\n",
        "            var_update = m_t / (math_ops.sqrt(v_t) + epsilon_t)\n",
        "\n",
        "        if self._initial_weight_decay > 0.0:\n",
        "            weight_decay = self._get_hyper('weight_decay', var_dtype)\n",
        "            var_update += weight_decay * var\n",
        "        var_update = state_ops.assign_sub(var, lr_t * var_update, use_locking=self._use_locking)\n",
        "\n",
        "        updates = [var_update, m_t, v_t]\n",
        "        if self.amsgrad:\n",
        "            updates.append(v_hat_t)\n",
        "        return control_flow_ops.group(*updates)\n",
        "\n",
        "    def _resource_apply_sparse(self, grad, var, indices):\n",
        "        var_dtype = var.dtype.base_dtype\n",
        "        lr_t = self._decayed_lr(var_dtype)\n",
        "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
        "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
        "        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n",
        "        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
        "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
        "\n",
        "        decay_steps = self._get_hyper('decay_steps', var_dtype)\n",
        "        warmup_steps = self._get_hyper('warmup_steps', var_dtype)\n",
        "        min_lr = self._get_hyper('min_lr', var_dtype)\n",
        "        lr_t = tf.where(\n",
        "            local_step <= warmup_steps,\n",
        "            lr_t * (local_step / warmup_steps),\n",
        "            min_lr + (lr_t - min_lr) * (1.0 - tf.minimum(local_step, decay_steps) / decay_steps),\n",
        "        )\n",
        "        lr_t = (lr_t * math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n",
        "\n",
        "        m = self.get_slot(var, 'm')\n",
        "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
        "        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n",
        "        with ops.control_dependencies([m_t]):\n",
        "            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
        "\n",
        "        v = self.get_slot(var, 'v')\n",
        "        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n",
        "        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n",
        "        with ops.control_dependencies([v_t]):\n",
        "            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
        "\n",
        "        if self.amsgrad:\n",
        "            v_hat = self.get_slot(var, 'vhat')\n",
        "            v_hat_t = math_ops.maximum(v_hat, v_t)\n",
        "            var_update = m_t / (math_ops.sqrt(v_hat_t) + epsilon_t)\n",
        "        else:\n",
        "            var_update = m_t / (math_ops.sqrt(v_t) + epsilon_t)\n",
        "\n",
        "        if self._initial_weight_decay > 0.0:\n",
        "            weight_decay = self._get_hyper('weight_decay', var_dtype)\n",
        "            var_update += weight_decay * var\n",
        "        var_update = state_ops.assign_sub(var, lr_t * var_update, use_locking=self._use_locking)\n",
        "\n",
        "        updates = [var_update, m_t, v_t]\n",
        "        if self.amsgrad:\n",
        "            updates.append(v_hat_t)\n",
        "        return control_flow_ops.group(*updates)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(AdamWarmup, self).get_config()\n",
        "        config.update({\n",
        "            'decay_steps': self._serialize_hyperparameter('decay_steps'),\n",
        "            'warmup_steps': self._serialize_hyperparameter('warmup_steps'),\n",
        "            'min_lr': self._serialize_hyperparameter('min_lr'),\n",
        "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
        "            'decay': self._serialize_hyperparameter('decay'),\n",
        "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
        "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
        "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
        "            'epsilon': self.epsilon,\n",
        "            'amsgrad': self.amsgrad,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "def calc_train_steps(num_example, batch_size, epochs, warmup_proportion=0.1):\n",
        "    \"\"\"Calculate the number of total and warmup steps.\n",
        "    (320, 32)\n",
        "    :param num_example: Number of examples in one epoch.\n",
        "    :param batch_size: Batch size.\n",
        "    :param epochs: Number of epochs.\n",
        "    :param warmup_proportion: The proportion of warmup steps.\n",
        "    :return: Total steps and warmup steps.\n",
        "    \"\"\"\n",
        "    steps = (num_example + batch_size - 1) // batch_size\n",
        "    total = steps * epochs\n",
        "    warmup = int(total * warmup_proportion)\n",
        "    return total, warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3_HfnsXCqVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpearmanRhoCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, training_data, validation_data, patience, model_name):\n",
        "        self.x = training_data[0]\n",
        "        self.y = training_data[1]\n",
        "        self.x_val = validation_data[0]\n",
        "        self.y_val = validation_data[1]\n",
        "\n",
        "        self.patience = patience\n",
        "        self.value = -1\n",
        "        self.bad_epochs = 0\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        y_pred_val = self.model.predict(self.x_val)\n",
        "        rho_val = np.mean([spearmanr(self.y_val[:, ind],\n",
        "                                     y_pred_val[:, ind]).correlation\n",
        "                           for ind in range(y_pred_val.shape[1])])\n",
        "        if rho_val >= self.value:\n",
        "            self.value = rho_val\n",
        "            self.bad_epochs = 0\n",
        "            self.model.save_weights(self.model_name)\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "        if self.bad_epochs >= self.patience:\n",
        "            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100 * ' ' + '\\n')\n",
        "        logs['val_rho'] = rho_val\n",
        "        return rho_val\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "import itertools\n",
        "\n",
        "def spearman_loss(y_true, y_pred):\n",
        "  return K.var(y_pred - y_true, axis=-1) / (K.std(y_pred, axis=-1)*K.std(y_true, axis=-1))# +K.random_uniform(shape=y_true.shape, minval=0.0, maxval=0.0001))\n",
        "\n",
        "def bert_model(bert_trainabel, learning_rate, len_tr, BATCH_SIZE, NUM_EPOCHS):\n",
        "    EMB_SIZE = 32\n",
        "\n",
        "    input_word_ids = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n",
        "    input_masks = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n",
        "    input_segments = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n",
        "\n",
        "    input_category = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_category')\n",
        "    input_host = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_host')\n",
        "\n",
        "    category_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n",
        "        tf.keras.layers.Embedding(input_dim=6, output_dim=EMB_SIZE)(input_category))\n",
        "    host_emb = tf.keras.layers.SpatialDropout1D(0.1)(\n",
        "        tf.keras.layers.Embedding(input_dim=65, output_dim=EMB_SIZE)(input_host))\n",
        "    features_dense = tf.keras.layers.concatenate([category_emb, host_emb], axis=1)\n",
        "    features_dense = tf.keras.layers.Flatten()(features_dense)\n",
        "\n",
        "    bert_layer = hub.KerasLayer(BERT_PATH, trainable=bert_trainabel)\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n",
        "\n",
        "    print('pooled_output: ', pooled_output)\n",
        "    print('sequence_output: ', sequence_output)\n",
        "\n",
        "    # bert_feat = tf.keras.layers.Dense(EMB_SIZE, activation=\"relu\")(pooled_output)\n",
        "    # fm_emb = [bert_feat, category_emb, host_emb]\n",
        "    # sum_add = tf.keras.layers.add(fm_emb)\n",
        "    # sum_add = tf.keras.layers.multiply([sum_add,sum_add])\n",
        "    # add_sum = []\n",
        "    # for layer in fm_emb:\n",
        "    #     add_sum.append(tf.keras.layers.multiply([layer,layer]))\n",
        "    # add_sum = tf.keras.layers.add(add_sum)\n",
        "        \n",
        "    # subtract_layer = tf.keras.layers.Lambda(lambda inputs: inputs[0] - inputs[1],output_shape=lambda shapes: shapes[0])\n",
        "    # fm_part = subtract_layer([sum_add, add_sum])\n",
        "    # fm_part  = tf.keras.layers.Lambda(lambda x: x * 0.5)(fm_part)\n",
        "    # fm_part = tf.keras.layers.Dropout(0.5)(fm_part)\n",
        "    # fm_part = tf.keras.layers.Flatten()(fm_part)\n",
        "    # print(fm_part)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.concatenate([x, features_dense])\n",
        "\n",
        "    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_masks, input_segments, input_category, input_host],\n",
        "                                  outputs=out)\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_tr,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "        )\n",
        "    adamW_opt = AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=learning_rate, min_lr=0,)\n",
        "    # Nadam = tf.keras.optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adamW_opt)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cu9-sQeCqbe",
        "colab_type": "code",
        "outputId": "b7967d28-3835-4181-d450-a32657854f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "# folds = GroupKFold(n_splits=5).split(X=df_train.question_title, groups=df_train.question_title)\n",
        "# folds = KFold(n_splits=3, shuffle=True, random_state=2019).split(outputs)\n",
        "kf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=0)\n",
        "idx = [x for x in kf.split(df_train, groups=df_train.question_title)]\n",
        "\n",
        "y_test = np.zeros((len(test_inputs), len(targets)))\n",
        "all_predictions = []\n",
        "score = []\n",
        "score2 = []\n",
        "score3 = []\n",
        "start_time = time.time()\n",
        "model_path = 'bert_weights_RepeatKfold'\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "# for fold_, (trn_idx, val_idx) in enumerate(folds):\n",
        "for fold_, (trn_idx, val_idx) in enumerate(idx[:2], 9):\n",
        "  print('fold_: ', fold_)\n",
        "#   if fold_<4:\n",
        "#     continue\n",
        "  K.clear_session()\n",
        "  \n",
        "  X_train = [inputs[i][trn_idx] for i in range(len(inputs))]\n",
        "  X_valid = [inputs[i][val_idx] for i in range(len(inputs))]\n",
        "  y_train = outputs[trn_idx]\n",
        "  y_valid = outputs[val_idx]\n",
        "\n",
        "  data_train = df_train.iloc[trn_idx]\n",
        "  data_valid = df_train.iloc[val_idx]\n",
        "  \n",
        "  model_weight = f'{model_path}/bert_fold{fold_}.h5'\n",
        "  model = bert_model(bert_trainabel=True, learning_rate=5e-5, len_tr=len(data_train), BATCH_SIZE=4, NUM_EPOCHS=4)\n",
        "  model.fit(X_train, y_train, epochs=4, batch_size=4,\n",
        "              callbacks=[SpearmanRhoCallback(training_data=(X_train, y_train), validation_data=(X_valid, y_valid),\n",
        "                                             patience=1, model_name=model_weight, ),\n",
        "                        # CyclicLR(base_lr=2e-5, max_lr=3e-5, step_size=300., mode='exp_range', gamma=0.99994)\n",
        "                        ])\n",
        "  \n",
        "  model = bert_model(bert_trainabel=False, learning_rate=1e-3, len_tr=len(data_train), BATCH_SIZE=32, NUM_EPOCHS=4)\n",
        "  model.load_weights(model_weight)\n",
        "  model.fit(X_train, y_train, epochs=4, batch_size=32,\n",
        "              callbacks=[SpearmanRhoCallback(training_data=(X_train, y_train), validation_data=(X_valid, y_valid),\n",
        "                                             patience=3, model_name=model_weight,),\n",
        "                        # CyclicLR(base_lr=1e-3, max_lr=2e-3, step_size=300., mode='exp_range', gamma=0.99994)\n",
        "                        ])\n",
        "  model.load_weights(model_weight)\n",
        "  \n",
        "  all_predictions.append(model.predict(test_inputs))\n",
        "  \n",
        "  valid_pred = model.predict(X_valid)\n",
        "  slice_score = np.mean([spearmanr(y_valid[:, ind], valid_pred[:, ind]).correlation\n",
        "                           for ind in range(valid_pred.shape[1])])\n",
        "  print('fold{} score: {}'.format(fold_, slice_score))\n",
        "\n",
        "  # valid_pred2 = np.round(valid_pred, 2)\n",
        "  # slice_score2 = np.mean([spearmanr(y_valid[:, ind], valid_pred2[:, ind]).correlation\n",
        "  #                          for ind in range(valid_pred2.shape[1])])\n",
        "  # print('fold{} score2: {}'.format(fold_, slice_score2))\n",
        "\n",
        "  # valid_pred3 = (valid_pred//(1/90))*(1/90)\n",
        "  # slice_score3 = np.mean([spearmanr(y_valid[:, ind], valid_pred3[:, ind]).correlation\n",
        "  #                          for ind in range(valid_pred3.shape[1])])\n",
        "  # print('fold{} score3: {}'.format(fold_, slice_score3))\n",
        "\n",
        "  score.append(slice_score)\n",
        "  # score2.append(slice_score2)\n",
        "  # score3.append(slice_score3)\n",
        "  print(\"time elapsed: {:<5.2}m\".format((time.time() - start_time) / 60))\n",
        "print('CV score: {}'.format(np.mean(score)))\n",
        "# print('CV score: {}'.format(np.mean(score2)))\n",
        "# print('CV score: {}'.format(np.mean(score3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold_:  9\n",
            "pooled_output:  Tensor(\"keras_layer/Identity:0\", shape=(None, 768), dtype=float32)\n",
            "sequence_output:  Tensor(\"keras_layer/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n",
            "Train on 4863 samples\n",
            "Epoch 1/4\n",
            "val_spearman-rho: 0.3712                                                                                                    \n",
            "4863/4863 [==============================] - 439s 90ms/sample - loss: 0.4115\n",
            "Epoch 2/4\n",
            "val_spearman-rho: 0.4019                                                                                                    \n",
            "4863/4863 [==============================] - 407s 84ms/sample - loss: 0.3655\n",
            "Epoch 3/4\n",
            "val_spearman-rho: 0.4122                                                                                                    \n",
            "4863/4863 [==============================] - 406s 84ms/sample - loss: 0.3411\n",
            "Epoch 4/4\n",
            "4860/4863 [============================>.] - ETA: 0s - loss: 0.3144Epoch 00003: early stopping Threshold\n",
            "val_spearman-rho: 0.4078                                                                                                    \n",
            "4863/4863 [==============================] - 405s 83ms/sample - loss: 0.3144\n",
            "pooled_output:  Tensor(\"keras_layer_1/Identity:0\", shape=(None, 768), dtype=float32)\n",
            "sequence_output:  Tensor(\"keras_layer_1/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n",
            "Train on 4863 samples\n",
            "Epoch 1/4\n",
            "val_spearman-rho: 0.4151                                                                                                    \n",
            "4863/4863 [==============================] - 144s 30ms/sample - loss: 0.3172\n",
            "Epoch 2/4\n",
            "val_spearman-rho: 0.4154                                                                                                    \n",
            "4863/4863 [==============================] - 135s 28ms/sample - loss: 0.3142\n",
            "Epoch 3/4\n",
            "val_spearman-rho: 0.4155                                                                                                    \n",
            "4863/4863 [==============================] - 135s 28ms/sample - loss: 0.3124\n",
            "Epoch 4/4\n",
            "val_spearman-rho: 0.4158                                                                                                    \n",
            "4863/4863 [==============================] - 135s 28ms/sample - loss: 0.3116\n",
            "fold9 score: 0.4157536902010747\n",
            "time elapsed: 3.8e+01m\n",
            "fold_:  10\n",
            "pooled_output:  Tensor(\"keras_layer/Identity:0\", shape=(None, 768), dtype=float32)\n",
            "sequence_output:  Tensor(\"keras_layer/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n",
            "Train on 4863 samples\n",
            "Epoch 1/4\n",
            "val_spearman-rho: 0.3672                                                                                                    \n",
            "4863/4863 [==============================] - 438s 90ms/sample - loss: 0.4051\n",
            "Epoch 2/4\n",
            "val_spearman-rho: 0.3954                                                                                                    \n",
            "4863/4863 [==============================] - 405s 83ms/sample - loss: 0.3633\n",
            "Epoch 3/4\n",
            "val_spearman-rho: 0.4003                                                                                                    \n",
            "4863/4863 [==============================] - 405s 83ms/sample - loss: 0.3394\n",
            "Epoch 4/4\n",
            "4860/4863 [============================>.] - ETA: 0s - loss: 0.3121Epoch 00003: early stopping Threshold\n",
            "val_spearman-rho: 0.3959                                                                                                    \n",
            "4863/4863 [==============================] - 403s 83ms/sample - loss: 0.3121\n",
            "pooled_output:  Tensor(\"keras_layer_1/Identity:0\", shape=(None, 768), dtype=float32)\n",
            "sequence_output:  Tensor(\"keras_layer_1/Identity_1:0\", shape=(None, None, 768), dtype=float32)\n",
            "Train on 4863 samples\n",
            "Epoch 1/4\n",
            "val_spearman-rho: 0.4032                                                                                                    \n",
            "4863/4863 [==============================] - 145s 30ms/sample - loss: 0.3146\n",
            "Epoch 2/4\n",
            "val_spearman-rho: 0.4032                                                                                                    \n",
            "4863/4863 [==============================] - 135s 28ms/sample - loss: 0.3113\n",
            "Epoch 3/4\n",
            "val_spearman-rho: 0.4048                                                                                                    \n",
            "4863/4863 [==============================] - 135s 28ms/sample - loss: 0.3097\n",
            "Epoch 4/4\n",
            "val_spearman-rho: 0.4048                                                                                                    \n",
            "4863/4863 [==============================] - 133s 27ms/sample - loss: 0.3090\n",
            "fold10 score: 0.40483598938929527\n",
            "time elapsed: 7.6e+01m\n",
            "CV score: 0.410294839795185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQl_DltZCqYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold0 score: 0.39553310346725007\n",
        "fold1 score: 0.4024302061634733\n",
        "fold2 score: 0.3911486329250454\n",
        "fold3 score: 0.38787471016487524\n",
        "fold4 score: 0.3958322311099684"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHVJ4jg_CqI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CV score: 0.39443920279732525"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng9ilJWXaNs6",
        "colab_type": "code",
        "outputId": "6057ecf5-8ed9-4c69-ccd7-c4a09986c726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(0.4028767893921821+0.4054496422760184+0.39370800478615+0.40809677256587584+0.39268607409936+0.39282118200606153+0.4053617546791114+0.40862098614678144+0.4157536902010747+0.40483598938929527) / 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40302108855419105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4FvLrcOt3nB",
        "colab_type": "code",
        "outputId": "8675e263-bd77-49b5-e880-78948bc5f1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(0.4028767893921821+0.4054496422760184+0.39370800478615+0.4157536902010747+0.40483598938929527) / 5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4045248232089441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76LXeuUbCqAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold1 score: 0.4028767893921821\n",
        "fold2 score: 0.4054496422760184\n",
        "fold3 score: 0.39370800478615\n",
        "fold4 score: 0.40809677256587584\n",
        "fold5 score: 0.39268607409936\n",
        "fold6 score: 0.39282118200606153\n",
        "fold7 score: 0.4053617546791114\n",
        "fold8 score: 0.40862098614678144\n",
        "fold9 score: 0.4157536902010747\n",
        "fold10 score: 0.40483598938929527\n",
        "CV score: 0.40302108855419105"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}