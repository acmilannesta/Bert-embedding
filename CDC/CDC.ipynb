{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/CDC/CDC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ItDxs5xGRfa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "6dc47e00-09d6-49dd-ef8d-5a2ba0e58f6b"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "!unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 18:28:17--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 2a00:1450:400c:c08::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  97.6MB/s    in 4.2s    \n",
            "\n",
            "2019-10-17 18:28:22 (91.8 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n",
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7T6jfH2s1wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f7a045ba-cb78-497a-ed87-49d268841975"
      },
      "source": [
        "!git clone https://github.com/acmilannesta/Bert-embedding"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Bert-embedding'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 67 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw-2XW8Geus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "c65c9f7c-17d7-4998-d32b-e74b96224c47"
      },
      "source": [
        "!pip install keras_bert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_bert) (1.16.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert) (2.2.5)\n",
            "Collecting keras-transformer>=0.30.0 (from keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (1.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.10.0 (from keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0 (from keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0 (from keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/f3/a92ce51219280eea003911722046db17eaebf5f26679a73887a5c357abe4/keras-layer-normalization-0.13.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0 (from keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0 (from keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0 (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras_bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=295549ac7378a58577b2107dca6378cc9c99867e087af4eb561a1420238a71cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=2376b401fe275bf32c72acc716396e9a38e57a4eab08102e40259a97a8d7ecb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=331ddd366dc6c4c200a33f76e6d27a0bb560da006e84ce4809374a1403b58b40\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=8aac22658f9ae5dc743a7b5669c6da3f7927f3d72a6a6a5aaf1b98a40e3f6391\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.13.0-cp36-none-any.whl size=5209 sha256=f13503bf915593544fd3b9b704f1fb4ae935bf9e8d9a0b629d7249e7ff6d707c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/2b/71/d1d06f71d78c46a9912dc89a5bb46f357cf64fa05883fadc64\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=57f8042dbfc05d1222db83b6b1fcf3c11a078938eb14e5f15dcdecc07d4a3ece\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=695a76aa6fa4e552032f6d3851a744b2cfaab247202ea1f7777cf11b2be98678\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=b789f11ce088157d04aa35a0c2c8ebd908fb439db6e33805d614a8fe022fe617\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.13.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "306WTFXmGPA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import choice\n",
        "from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps\n",
        "import re, os, gc\n",
        "import codecs\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJhMtLYVGl0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 142 #@param {type:'slider', min:50, max:300, step:1}\n",
        "BATCH_SIZE = 16 #@param {type:'slider', min:8, max:32, step:8}\n",
        "config_path = 'uncased_L-12_H-768_A-12/bert_config.json' #@param ['uncased_L-12_H-768_A-12/bert_config.json', 'drive/My Drive/biobert_pretrain_output_all_notes_150000/bert_config.json']\n",
        "checkpoint_path = 'uncased_L-12_H-768_A-12/bert_model.ckpt' #@param ['uncased_L-12_H-768_A-12/bert_model.ckpt', '/drive/My Drive/biobert_pretrain_output_all_notes_150000/model.ckpt']\n",
        "dict_path = 'uncased_L-12_H-768_A-12/vocab.txt' #@param ['uncased_L-12_H-768_A-12/vocab.txt', 'drive/My Drive/biobert_pretrain_output_all_notes_150000/vocab.txt']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HC_n547GuYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "  for line in reader:\n",
        "      token = line.strip()\n",
        "      token_dict[token] = len(token_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r973L4toG4s3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class OurTokenizer(Tokenizer):\n",
        "#     def _tokenize(self, text):\n",
        "#         R = []\n",
        "#         for c in text:\n",
        "#             if c in self._token_dict:\n",
        "#                 R.append(c)\n",
        "#             # elif self._is_space(c):\n",
        "#             #     R.append('[unused1]')\n",
        "#             else:\n",
        "#                 R.append('[UNK]')\n",
        "#         return R\n",
        "\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQK47tDfHL-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('Bert-embedding/CDC/train.csv')\n",
        "test = pd.read_csv('Bert-embedding/CDC/test.csv')\n",
        "wt = pd.DataFrame(train.event.value_counts()/len(train)).rename(columns={'event':'weight'})\n",
        "wt['event'] = wt.index\n",
        "train = train.merge(wt, how='left', on='event')\n",
        "train['event_idx'] = train.event.map({y:x for x, y in enumerate(np.sort(train.event.unique()))})\n",
        "# X_train, X_val, y_train, y_val = train_test_split(train[['text', 'weight']], train['event'], test_size=0.2, random_state=0)\n",
        "# train_data = list(zip(X_train['text'], X_train['weight'], y_train))\n",
        "# valid_data = list(zip(X_val['text'], X_val['weight'], y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLKGXJQEZ3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, targets = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        ids, segments = tokenizer.encode(data_df['text'][i], max_len=142)\n",
        "        indices.append(ids)\n",
        "        targets.append(data_df['event_idx'][i])\n",
        "    items = list(zip(indices, targets))\n",
        "    np.random.shuffle(items)\n",
        "    indices, targets = zip(*items)\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)], np.array(targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_uDtNyE7UV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a5fab4d-72d8-4193-b1b6-5e782b3a337a"
      },
      "source": [
        "train_x, train_y = convert_data(train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 153956/153956 [00:42<00:00, 3634.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl-tLqJGHXij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def seq_padding(X, padding=0):\n",
        "#   L = [len(x) for x in X]\n",
        "#   ML = max(L)\n",
        "#   return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X])\n",
        "\n",
        "# class data_generator:\n",
        "#   def __init__(self, data, batch_size=BATCH_SIZE, maxlen=MAXLEN):\n",
        "#     self.data = data\n",
        "#     self.batch_size = batch_size\n",
        "#     self.maxlen = maxlen\n",
        "#     self.steps = len(self.data) // self.batch_size\n",
        "#     if len(self.data) % self.batch_size != 0:\n",
        "#         self.steps += 1\n",
        "#   def __len__(self):\n",
        "#     return self.steps\n",
        "#   def __iter__(self):\n",
        "#     while True:\n",
        "#       idxs = list(range(len(self.data)))\n",
        "#       np.random.shuffle(idxs)\n",
        "#       X1, X2, W, Y = [], [], [], []\n",
        "#       for i in idxs:\n",
        "#         d = self.data[i]\n",
        "#         text = d[0][:self.maxlen]\n",
        "#         x1, x2 = tokenizer.encode(first=text)\n",
        "#         w = d[1]\n",
        "#         y = d[2]\n",
        "#         X1.append(x1)\n",
        "#         X2.append(x2)\n",
        "#         W.append(w)\n",
        "#         Y.append([y])\n",
        "#         if len(X1) == self.batch_size or i == idxs[-1]:\n",
        "#           X1 = seq_padding(X1)\n",
        "#           X2 = seq_padding(X2)\n",
        "#           Y = seq_padding(Y)\n",
        "#           yield [X1, X2, np.array(W)], Y\n",
        "#           [X1, X2, W, Y] = [], [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnY5nGsHgHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    seq_len = MAXLEN,\n",
        "    trainable=True)\n",
        "\n",
        "# x1_in = Input(shape=(None,))\n",
        "# x2_in = Input(shape=(None,))\n",
        "# w_in = Input(shape=(1,))\n",
        "inputs = bert_model.inputs[:2]\n",
        "dense = bert_model.layers[-3].output\n",
        "# x = bert_model([x1_in, x2_in])\n",
        "# x = Lambda(lambda x: x[:, 0])(x)\n",
        "outputs = Dense(48, activation='softmax')(dense)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# model = Model([x1_in, x2_in], p)\n",
        "# def f1(weights):\n",
        "#     def metric(true_labels, predictions):\n",
        "#         f1 = tf.contrib.metrics.f1_score(true_labels, predictions, weights=weights)[1]\n",
        "#         K.get_session().run(tf.local_variables_initializer())\n",
        "#         return f1\n",
        "#     return metric\n",
        "# decay_steps, warmup_steps = calc_train_steps(\n",
        "#     len(train_data),\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     epochs=2,\n",
        "# )\n",
        "loss = \"sparse_categorical_crossentropy\" #@param ['sparse_categorical_crossentropy', 'categroical_crossentropy']\n",
        "metrics = \"sparse_categorical_accuracy\" #@param ['sparse_categorical_accuracy', 'acc']\n",
        "model.compile(\n",
        "    loss=loss,\n",
        "    optimizer=SGD(lr=1e-4), #Adam(1e-5),\n",
        "    # optimizer=AdamWarmup(decay_steps=decay_steps, warmup_steps=warmup_steps, lr=1e-5),\n",
        "    metrics= [metrics]\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuSl9xDAGYZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "7e44437e-4b11-401a-f2d4-3d4c72934f69"
      },
      "source": [
        "model.fit(train_x, train_y, batch_size=16, epochs=2, validation_split=0.2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c2aa2f8aaa9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected Input-Token to have shape (512,) but got array with shape (142,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOtLEfvSLWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict([x[500:600] for x in train_x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukmrzfl5Sbph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "7d14256f-ed50-42fd-db3d-be10f7695bc5"
      },
      "source": [
        "pred"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       ...,\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31l7HU8ZHsAe",
        "colab_type": "code",
        "outputId": "0c3d27af-a38d-4865-c346-50cda18733c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train_D = data_generator(train_data[:500])\n",
        "valid_D = data_generator(valid_data[:500])\n",
        "model.fit_generator(\n",
        "    train_D.__iter__(),\n",
        "    steps_per_epoch=len(train_D),\n",
        "    epochs=2,\n",
        "    validation_data=valid_D.__iter__(),\n",
        "    validation_steps=len(valid_D)\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "32/32 [==============================] - 24s 751ms/step - loss: nan - sparse_categorical_accuracy: 0.0000e+00 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "32/32 [==============================] - 21s 661ms/step - loss: nan - sparse_categorical_accuracy: 0.0000e+00 - val_loss: nan - val_sparse_categorical_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe4c2edbe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAj2g9x1DwTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5351e42-dea5-4db4-9204-810368bed34d"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2frFKaglPXeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict_generator(valid_D.__iter__(), len(valid_D))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VSNpioADeT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "9ef641ce-3771-422a-a6df-3f3effc7efee"
      },
      "source": [
        "pred"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       ...,\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan],\n",
              "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn5G__2CL2Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('ROC-AUC: {:.2f}%'.format(roc_auc_score([x[1] for x in valid_data], pred)*100))\n",
        "print('Accuracy Score: {:.2f}%'.format(accuracy_score([x[1] for x in valid_data], [int(x>0.5) for x in pred])*100))\n",
        "print('Precision Score: {:.2f}%'.format(precision_score([x[1] for x in valid_data], [int(x>0.5) for x in pred])*100))\n",
        "print('Recall Score: {:.2f}%'.format(recall_score([x[1] for x in valid_data], [int(x>0.5) for x in pred])*100))\n",
        "print('F1 Score: {:.2f}%'.format(f1_score([x[1] for x in valid_data], [int(x>0.5) for x in pred])*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}