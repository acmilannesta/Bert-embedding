{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/Toxic_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKhYbkkRwQ9I",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "33af5e4a-d4b0-4881-dfde-fb71c39cfd74"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a05951c3-09ad-4b66-8e94-dad53520ee5e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a05951c3-09ad-4b66-8e94-dad53520ee5e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktU5oajByf3l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "43e315fa-52ca-41d4-d8d1-a7b4493d78e8"
      },
      "source": [
        "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/221k [00:00<?, ?B/s]\n",
            "\r100% 221k/221k [00:00<00:00, 71.3MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 41% 5.00M/12.1M [00:00<00:00, 43.9MB/s]\n",
            "100% 12.1M/12.1M [00:00<00:00, 76.8MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 95% 260M/273M [00:01<00:00, 153MB/s]\n",
            "100% 273M/273M [00:01<00:00, 143MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsECSOvhuCQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFfFwAv9sh2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate, CuDNNLSTM, CuDNNGRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras.utils import Sequence\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMyGXNGGs6BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 220\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 4\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 512\n",
        "NUM_MODELS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sUcnLAp32KS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0179f0cb-febd-4fe3-b4f0-e12b36890753"
      },
      "source": [
        "train = pd.read_csv('train.csv.zip')\n",
        "test = pd.read_csv('test.csv.zip')\n",
        "# preprocess text file\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "def text_prepare(text):\n",
        "    for p in punct:\n",
        "        text = text.replace(p, ' ')\n",
        "    return text.split()\n",
        "train_text = [text_prepare(row) for row in tqdm(train['comment_text'])]\n",
        "test_text = [text_prepare(row) for row in tqdm(test['comment_text'])]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [00:45<00:00, 39917.62it/s]\n",
            "100%|██████████| 97320/97320 [00:01<00:00, 53981.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9p6gVMovm2I",
        "colab_type": "code",
        "outputId": "e775a7c5-7b51-401b-be99-1dba177ae0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "identity_columns = [\n",
        "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
        "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
        "# Overall\n",
        "weights = np.ones((len(train),)) / 4\n",
        "# Subgroup\n",
        "weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n",
        "# Background Positive, Subgroup Negative\n",
        "weights += (( (train['target'].values>=0.5).astype(np.int) +\n",
        "   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(np.int) / 4\n",
        "# Background Negative, Subgroup Positive\n",
        "weights += (( (train['target'].values<0.5).astype(np.int) +\n",
        "   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(np.int) / 4\n",
        "# loss_weight = 1.0 / weights.mean()\n",
        "y_train = (train['target'].values >= 0.5).astype(np.int)\n",
        "# y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n",
        "del train\n",
        "gc.collect()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUK6HMoi4LK5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5e065fde-666b-4538-b827-7dd3e78f8090"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tqdm(train_text+test_text))\n",
        "X_train = tokenizer.texts_to_sequences(tqdm(train_text))\n",
        "X_test = tokenizer.texts_to_sequences(test_text)\n",
        "del train_text, test_text, tokenizer\n",
        "gc.collect()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1902194/1902194 [01:19<00:00, 23892.23it/s]\n",
            "100%|██████████| 1804874/1804874 [01:04<00:00, 27866.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaKpWSBLwcW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train = np.load('drive/My Drive/Data/X_train.npy')\n",
        "# X_test = np.load('drive/My Drive/Data/X_test.npy')\n",
        "embedding_matrix = np.load('drive/My Drive/Data/crawl+glove.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8_X5OZ86s_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr, X_val, y_tr, y_val, w_tr, w_val = train_test_split(X_train, y_train, weights, stratify=y_train, test_size=0.01, random_state=2019)\n",
        "class TrainSeq(Sequence):\n",
        "\n",
        "    def __init__(self, batch_size=512, percentile=95):\n",
        "        self.x, self.y, self.w = X_tr, y_tr, w_tr\n",
        "        self.batch_size = batch_size\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_w = self.w[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        length = min(int(np.percentile([len(o) for o in batch_x], self.percentile)), MAX_LEN)\n",
        "        batch_x = np.array(pad_sequences(batch_x, length, padding='post'))\n",
        "        return {model.input_names[0]: batch_x, model.input_names[1]: batch_w}, batch_y, batch_w\n",
        "class ValSeq(Sequence):\n",
        "\n",
        "    def __init__(self, batch_size=512, percentile=95):\n",
        "        self.x, self.y, self.w = X_val, y_val, w_val\n",
        "        self.batch_size = batch_size\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_w = self.w[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        length = min(int(np.percentile([len(o) for o in batch_x], self.percentile)), MAX_LEN)\n",
        "        batch_x = np.array(pad_sequences(batch_x, length, padding='post')).reshape(-1, length)\n",
        "        return {model.input_names[0]: batch_x, model.input_names[1]: batch_w}, batch_y, batch_w\n",
        "      \n",
        "class PredSeq(Sequence):\n",
        "\n",
        "    def __init__(self, batch_size=2048, percentile=95):\n",
        "        self.x = X_test\n",
        "        self.batch_size = batch_size\n",
        "        self.percentile = percentile\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        length = min(int(np.percentile([len(o) for o in batch_x], self.percentile)), MAX_LEN)\n",
        "        batch_x = np.array(pad_sequences(batch_x, length, padding='post'))\n",
        "        batch_w = np.ones(self.batch_size)\n",
        "        return {model.input_names[0]: batch_x, model.input_names[1]: batch_w}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6tXPOG_z0fg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auc(weights):\n",
        "    def metric(y_true, y_pred):\n",
        "        auc = tf.metrics.auc(y_true, y_pred, weights=weights)[1]\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        return auc\n",
        "    return metric\n",
        "# def custom_loss(y_true, y_pred):\n",
        "#     return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n",
        "def build_model(embedding_matrix):\n",
        "\n",
        "    words = Input(shape=(None, ))\n",
        "    weight = Input(shape=(1,))\n",
        "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
        "    x = SpatialDropout1D(0.3)(x)\n",
        "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x), ])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(1, activation='sigmoid')(hidden)\n",
        "#     aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
        "\n",
        "    model = Model(inputs=[words, weight], outputs=result)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  # loss_weights=[loss_weight, 1.0],\n",
        "                  metrics=[auc(weight)],\n",
        "                  optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMQ2h-Xe0TRJ",
        "colab_type": "code",
        "outputId": "5316f594-12d8-47bb-de37-221335e255f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "checkpoint_predictions = []\n",
        "weights = []\n",
        "for model_idx in range(NUM_MODELS):\n",
        "    model = build_model(embedding_matrix)\n",
        "    for global_epoch in range(4):\n",
        "        model.fit_generator(TrainSeq(), \n",
        "                            validation_data=ValSeq(), \n",
        "                            verbose=1,\n",
        "                            epochs=1,\n",
        "                            callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))]\n",
        "                           )\n",
        "#             model.fit(\n",
        "#                 [X_tr, y_tr[:, 1]], y_tr,\n",
        "#                 batch_size=BATCH_SIZE,\n",
        "#                 epochs=1,\n",
        "#                 verbose=1,\n",
        "#                 validation_data=([X_val, y_val[:, 1]], y_val),\n",
        "#                 callbacks=[LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))]\n",
        "#             )\n",
        "        checkpoint_predictions.append(model.predict_generator(PredSeq()).flatten())\n",
        "        weights.append(2 ** global_epoch)\n",
        "    del model\n",
        "    gc.collect()\n",
        "\n",
        "preds = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
        "# return preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "3490/3490 [==============================] - 729s 209ms/step - loss: 0.0675 - metric: 0.8964 - val_loss: 0.0608 - val_metric: 0.9237\n",
            "Epoch 1/1\n",
            "3490/3490 [==============================] - 732s 210ms/step - loss: 0.0583 - metric: 0.9308 - val_loss: 0.0554 - val_metric: 0.9354\n",
            "Epoch 1/1\n",
            "3490/3490 [==============================] - 732s 210ms/step - loss: 0.0548 - metric: 0.9390 - val_loss: 0.0556 - val_metric: 0.9418\n",
            "Epoch 1/1\n",
            "1468/3490 [===========>..................] - ETA: 7:03 - loss: 0.0523 - metric: 0.9430"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}