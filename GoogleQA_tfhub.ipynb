{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleQA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/GoogleQA_tfhub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82bieSpLcuc",
        "colab_type": "code",
        "outputId": "44a996fe-4368-46be-be49-f8fedb9f13eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan 16 15:38:29 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErzYVOxLtWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlp aug\n",
        "!pip install nlpaug transformers\n",
        "\n",
        "# bert uncased base\n",
        "!wget https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1?tf-hub-format=compressed -O bbu\n",
        "!mkdir bert_base_uncased\n",
        "!tar -xzf bbu -C bert_base_uncased\n",
        "\n",
        "# bert cased base\n",
        "!wget https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1?tf-hub-format=compressed -O bbc\n",
        "!mkdir bert_base_cased\n",
        "!tar -xzf bbc -C bert_base_cased\n",
        "\n",
        "# albert base\n",
        "!wget https://tfhub.dev/google/albert_base/1?tf-hub-format=compressed -O ab\n",
        "!mkdir albert_base\n",
        "!tar -xzf ab -C albert_base\n",
        "\n",
        "# bert uncased large\n",
        "!wget https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1?tf-hub-format=compressed -O blu\n",
        "!mkdir bert_large_uncased\n",
        "!tar -xzf blu -C bert_large_uncased\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBrwK8dwLyw8",
        "colab_type": "code",
        "outputId": "c0e0f4a1-aec6-40dd-8ffc-fb760d8edc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaDm4NWvLp_J",
        "colab_type": "code",
        "outputId": "5091f764-8af7-41dd-c8f4-59a7fdb217e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import sys\n",
        "sys.path.extend(['/content/drive/My Drive/GoogleQA'])\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, gc, codecs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold, RepeatedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "# from albert_tokenization import FullTokenizer\n",
        "import bert_tokenizer\n",
        "from warmup_v2 import AdamWarmup, calc_train_steps\n",
        "# import optimizer_lamb\n",
        "tf.get_logger().setLevel('ERROR') \n",
        "from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n",
        "from functools import partial\n",
        "from scipy.stats import rankdata\n",
        "from tensorflow.python.framework import ops\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import transformers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4yFl6cbgX6I",
        "colab_type": "text"
      },
      "source": [
        "##Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xGojs4dL6S4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'drive/My Drive/GoogleQA/Data/'\n",
        "# aux_path = 'drive/My Drive/GoogleQA/Web Scrap/'\n",
        "train = pd.read_csv(data_path+'train.csv')\n",
        "test = pd.read_csv(data_path+'test.csv')\n",
        "target_col = train.columns.tolist()[11:42]\n",
        "q_col, a_col = target_col[:21], target_col[21:]\n",
        "\n",
        "\n",
        "# Label encoding category and host\n",
        "class MyLabelEncoder(object):\n",
        "    \"\"\"safely handle unknown label\"\"\"\n",
        "    def __init__(self):\n",
        "        self.mapper = {}\n",
        "    def fit(self, X):\n",
        "        uniq_X = np.unique(X)\n",
        "        # reserve 0 for unknown\n",
        "        self.mapper = dict(zip(uniq_X, range(1, len(uniq_X) + 1)))\n",
        "        return self\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "    def _map(self, x):\n",
        "        return self.mapper.get(x, 0)\n",
        "    def transform(self, X):\n",
        "        return list(map(self._map, X))\n",
        "\n",
        "category_encoder = MyLabelEncoder().fit(train['category'])\n",
        "train['category'] = category_encoder.transform(train['category'])\n",
        "test['category'] = category_encoder.transform(test['category'])\n",
        "host_encoder = MyLabelEncoder().fit(train['host'])\n",
        "train['host'] = host_encoder.transform(train['host'])\n",
        "test['host'] = host_encoder.transform(test['host'])\n",
        "\n",
        "# average question targets within the same questions\n",
        "def q_avg(data_df):\n",
        "    q_col_med = data_df.groupby(['question_title'], as_index=False)[q_col].mean()\n",
        "    return data_df.drop(target_col, 1).merge(q_col_med, how='left', on='question_title').drop_duplicates('question_title').reset_index(drop=True)\n",
        "\n",
        "# assign class weights\n",
        "def label_wt(data_df, cols, wt1=2, wt2=1.5, wt3=1):\n",
        "    for col in cols:\n",
        "        wt = data_df[col].value_counts(normalize=True).rename(col+'_wt').to_frame()\n",
        "        wt[col+'_wt'] = np.where(wt[col+'_wt']<=0.01, wt1, np.where(wt[col+'_wt']<=0.05, wt2, wt3))\n",
        "        data_df = data_df.merge(wt, 'left', left_on=col, right_on=wt.index)\n",
        "    return data_df\n",
        "\n",
        "# train, train_q, train_a = label_wt(train, target_col), label_wt(train_q, q_col), label_wt(train_a, a_col)\n",
        "col_wt, q_col_wt, a_col_wt = [x+'_wt' for x in target_col], [x+'_wt' for x in q_col], [x+'_wt' for x in a_col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7QNQl1wDJbi",
        "colab_type": "text"
      },
      "source": [
        "## Parameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHK5JvMJpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 512 \n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 3\n",
        "NUM_CLASSES = 30\n",
        "NUM_AUX = 6\n",
        "LR = 5e-5\n",
        "MIN_LR = 0\n",
        "model_path = 'drive/My Drive/GoogleQA/bert_base_uncased' \n",
        "save_path = 'bbu10'\n",
        "save_model = 'bbu10' \n",
        "CASED = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MknUuh8pFpq0",
        "colab_type": "text"
      },
      "source": [
        "## Bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWJBpOFN7FCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _trim_input(title, question, answer, max_sequence_length=MAXLEN,\n",
        "                t_max_len=15, q_max_len=245, a_max_len=248):\n",
        "    t_len = len(title)\n",
        "    q_len = len(question)\n",
        "    a_len = len(answer)\n",
        "    if (t_len + q_len + a_len +6) > max_sequence_length:\n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + np.floor((t_max_len - t_len) / 2)\n",
        "            q_max_len = q_max_len + np.ceil((t_max_len - t_len) / 2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len\n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "        if t_new_len + a_new_len + q_new_len +6 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\"\n",
        "                    % (max_sequence_length, (t_new_len + a_new_len + q_new_len +6)))\n",
        "        head_t_new_len = int(0.25 * t_new_len)\n",
        "        tail_t_new_len = int(t_new_len - head_t_new_len)\n",
        "\n",
        "        head_q_new_len = int(0.25 * q_new_len)\n",
        "        tail_q_new_len = int(q_new_len - head_q_new_len)\n",
        "\n",
        "        head_a_new_len = int(0.25 * a_new_len)\n",
        "        tail_a_new_len = int(a_new_len - head_a_new_len)\n",
        "\n",
        "        title = title[:head_t_new_len] + title[-tail_t_new_len:]\n",
        "        question = question[:head_q_new_len] + question[-tail_q_new_len:]\n",
        "        answer = answer[:head_a_new_len] + answer[-tail_a_new_len:]\n",
        "\n",
        "    return title, question, answer\n",
        "\n",
        "tokenizer = bert_tokenizer.FullTokenizer(os.path.join(model_path, 'assets/vocab.txt'))\n",
        "for token in ['[QT]', '[QB]', '[AN]']:\n",
        "    tokenizer.vocab[token] = len(tokenizer.vocab)\n",
        "# for category in np.unique(train['category']):\n",
        "#     tokenizer.vocab[f'[{category}]'] = len(tokenizer.vocab)\n",
        "# for host in np.unique(train['host']):\n",
        "#     tokenizer.vocab[f'[{host}]'] = len(tokenizer.vocab)\n",
        "# tokenizer.vocab['[category_unk]'] = len(tokenizer.vocab)\n",
        "# tokenizer.vocab['[host_unk]'] = len(tokenizer.vocab)\n",
        "\n",
        "def q_trim(tokens, max_length=MAXLEN):\n",
        "# def q_trim(q_, a_, max_length=MAXLEN):\n",
        "    if len(tokens) > max_length:\n",
        "        head_length = int(0.25 * max_length)\n",
        "        tail_length = max_length - head_length\n",
        "        tokens = tokens[:head_length] + tokens[-tail_length:]\n",
        "    return tokens\n",
        "    # if len(q_) >= int(max_length*0.7):\n",
        "    #     head_length = int(0.25 * int(max_length*0.7))\n",
        "    #     tail_length = int(max_length*0.7) - head_length\n",
        "    #     q_ = q_[:head_length] + q_[-tail_length:]\n",
        "\n",
        "    # if len(q_) + len(a_) > max_length:\n",
        "    #     a_length = max_length - len(q_)\n",
        "    #     head_a_length = int(0.25*a_length)\n",
        "    #     tail_a_length = a_length - head_a_length\n",
        "    #     a_ = a_[:head_a_length] + a_[-tail_a_length:]\n",
        "    # return q_, a_\n",
        "\n",
        "def a_trim(q_, a_, max_length=MAXLEN):\n",
        "    a_max_length = int(max_length*0.75)\n",
        "    if len(a_) >= a_max_length:\n",
        "        head_length = int(0.25 * a_max_length)\n",
        "        tail_length = a_max_length - head_length\n",
        "        a_ = a_[:head_length] + a_[-tail_length:]\n",
        "\n",
        "    if len(q_) + len(a_) > max_length:\n",
        "        q_length = max_length - len(a_)\n",
        "        head_q_length = int(0.25*q_length)\n",
        "        tail_q_length = q_length - head_q_length\n",
        "        q_ = q_[:head_q_length] + q_[-tail_q_length:]\n",
        "    return q_, a_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHGdTW66nTQ",
        "colab_type": "text"
      },
      "source": [
        "## Convert data to inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2llbNDMhVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df, model, targets, branch='training'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    global MAXLEN\n",
        "    global target_col\n",
        "    ids, segments = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        a = tokenizer.tokenize(data_df.loc[i, 'question_title'])\n",
        "        b = tokenizer.tokenize(data_df.loc[i, 'question_body'])\n",
        "        c = tokenizer.tokenize(data_df.loc[i, 'answer'])\n",
        "        if model == 'qa':\n",
        "            a, b, c = _trim_input(a, b, c, t_max_len=15, q_max_len=244, a_max_len=247)\n",
        "            question = tokenizer.convert_tokens_to_ids(['[CLS]']+['[QT]']+a+['[QB]']+b+['[SEP]'])\n",
        "            answer = tokenizer.convert_tokens_to_ids(['[AN]']+c+['[SEP]'])\n",
        "            ids.append(question+answer)\n",
        "            segments.append([0] * len(question) + [1] * len(answer))\n",
        "        if model == 'q':\n",
        "            question = tokenizer.convert_tokens_to_ids(['[CLS]']+['[QT]']+a+['[SEP]']+['[QB]']+b+['[SEP]'])\n",
        "            # answer = tokenizer.convert_tokens_to_ids(['[AN]']+c+['[SEP]'])\n",
        "            question_trim = q_trim(question)\n",
        "            ids.append(question_trim)\n",
        "            segments.append([0] * len(question_trim))\n",
        "        if model == 'a':\n",
        "            question = tokenizer.convert_tokens_to_ids(['[CLS]']+['[QT]']+a+['[QB]']+b+['[SEP]'])\n",
        "            answer = tokenizer.convert_tokens_to_ids(['[AN]']+c+['[SEP]'])         \n",
        "            question_trim, answer_trim = a_trim(question, answer)\n",
        "            ids.append(question_trim + answer_trim)\n",
        "            segments.append([0] * len(question_trim) + [1] * len(answer_trim))\n",
        "    aux = data_df[['category', 'host']]    \n",
        "    if branch == 'training':\n",
        "        targets = data_df[targets]\n",
        "        return [ids, segments], np.array(aux, dtype='int32'), np.array(targets)\n",
        "        # return [ids, segments], np.array(targets)\n",
        "    else:\n",
        "        return [ids, segments], np.array(aux, dtype='int32')\n",
        "        # return [ids, segments]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7CUvw-fx08h",
        "colab_type": "text"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7qwb8ZMlbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Data Generator\"\"\"\n",
        "def seq_padding(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = min(max(L), MAXLEN)\n",
        "    # ML = MAXLEN\n",
        "    out = np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x[:ML] for x in X])\n",
        "    return tf.convert_to_tensor(out, dtype=tf.int32)\n",
        "    \n",
        "def get_masks(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = min(max(L), MAXLEN)\n",
        "    # ML = MAXLEN\n",
        "    out = np.array([np.concatenate([[1]*len(x), [padding] * (ML - len(x))]) if len(x) < ML else [1]*ML for x in X])\n",
        "    return tf.convert_to_tensor(out, dtype=tf.int32)\n",
        "\n",
        "class data_generator:\n",
        "    def __init__(self, data, batch_size=BATCH_SIZE, branch='train', multi_target=True):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.branch = branch\n",
        "        self.steps = len(self.data) // self.batch_size\n",
        "        self.multi_target=multi_target\n",
        "        if len(self.data) % self.batch_size != 0:\n",
        "            self.steps += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if self.branch == 'train':\n",
        "                np.random.shuffle(self.data)\n",
        "            for i in range(self.steps):\n",
        "                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "                X1 = seq_padding([x[0] for x in d])\n",
        "                X2 = get_masks([x[0] for x in d])    \n",
        "                X3 = seq_padding([x[1] for x in d])\n",
        "\n",
        "                aux = np.array([x[2][0] for x in d]).reshape(-1, 1)\n",
        "                aux = tf.convert_to_tensor(aux, dtype=tf.int32)\n",
        "                aux2 = np.array([x[2][1] for x in d]).reshape(-1, 1)\n",
        "                aux2 = tf.convert_to_tensor(aux2, dtype=tf.int32)\n",
        "                if self.branch != 'train':\n",
        "                    yield [X1, X2, X3, aux, aux2]\n",
        "                else:                    \n",
        "                    if self.multi_target:\n",
        "                        Y = []\n",
        "                        num_targets = int(d[0][3].shape[0] / 2)\n",
        "                        for i in range(num_targets):\n",
        "                            Y.append(tf.convert_to_tensor([x[3][[i, i+num_targets]] for x in d], dtype=tf.float32))\n",
        "                    else:\n",
        "                        Y = tf.convert_to_tensor([x[3] for x in d], dtype=tf.float32)\n",
        "                    yield [X1, X2, X3, aux, aux2], Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwdZXD38axV",
        "colab_type": "text"
      },
      "source": [
        "## Build model structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvVEWF62IdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.custom_gradient\n",
        "def transfer(x):\n",
        "    def grad_fn(dy):\n",
        "        return dy\n",
        "    return tf.floor(x*1000)/1000, grad_fn\n",
        "\n",
        "# Define custom py_func which takes also a grad op as argument:\n",
        "def py_func(func, inp, Tout, name=None, grad=None):\n",
        "    \n",
        "    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))\n",
        "    \n",
        "    tf.RegisterGradient(rnd_name)(grad)  \n",
        "    g = tf.compat.v1.get_default_graph()\n",
        "    with g.gradient_override_map({\"PyFunc\": rnd_name}):\n",
        "        return tf.py_function(func, inp, Tout, name=name)\n",
        "\n",
        "# Def custom square function using np.square instead of tf.square:\n",
        "def myrankdata(x, name=None):\n",
        "    \n",
        "    with ops.op_scope([x], name, \"MyGrad\") as name:\n",
        "        rank = py_func(rankdata,\n",
        "                        [x],\n",
        "                        [tf.float32],\n",
        "                        name=name,\n",
        "                        grad=_MyGrad)  \n",
        "        return rank\n",
        "\n",
        "# Actual gradient:\n",
        "def _MyGrad(op, grad):\n",
        "    return grad * op.inputs[0]  \n",
        "\n",
        "def custom_spearman(multi_target):\n",
        "    def rhos(y_true, y_pred):\n",
        "        a = y_true[:, 0]\n",
        "        b = y_pred\n",
        "        rank_a, rank_b = myrankdata(a)[0], myrankdata(b)[0]\n",
        "        rho = 1 - 6 * tf.reduce_sum((rank_a - rank_b) ** 2) / (BATCH_SIZE ** 3 - BATCH_SIZE)\n",
        "        rho = K.maximum(rho, -1.0)\n",
        "        if multi_target:\n",
        "            return 1 - K.square(rho) #* y_true[:, 1]\n",
        "        else:\n",
        "            return 1-rho\n",
        "    return rhos\n",
        "\n",
        "def custom_pearson(multi_target):\n",
        "    def rhos(y_true, y_pred):\n",
        "        xm, ym = y_true-K.mean(y_true), y_pred-K.mean(y_pred)\n",
        "        r_num = K.sum(tf.multiply(xm, ym))\n",
        "        r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "        r = r_num / r_den\n",
        "        r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "        if multi_target:\n",
        "            return (1 - K.square(r)) * y_true[:, 1]\n",
        "        else:\n",
        "            return 1 - K.square(r)\n",
        "    return rhos\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return keras.metrics.binary_crossentropy(tf.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n",
        "\n",
        "def model_build(len_train, num_targets, dropout=0.2, lr=LR, epochs=NUM_EPOCHS, bert_trainable=True, multi_target=True):\n",
        "    global NUM_CLASSES\n",
        "    global BATCH_SIZE\n",
        "    global NUM_EPOCHS\n",
        "    global MIN_LR\n",
        "    # global LR\n",
        "    global MAXLEN\n",
        "    global model_path\n",
        "    global NUM_AUX\n",
        " \n",
        "    q_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_input_word_ids\")\n",
        "    q2_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_input_masks\")\n",
        "    q3_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_segment_ids\")\n",
        "    input_category = keras.layers.Input((1,), dtype=tf.int32, name='input_category')\n",
        "    input_host = tf.keras.layers.Input((1,), dtype=tf.int32, name='input_host')\n",
        "\n",
        "    bert_layer = hub.KerasLayer(model_path, trainable=bert_trainable)\n",
        "    _, q_inputs  = bert_layer([q_in, q2_in, q3_in])\n",
        "    q_inputs = keras.layers.Lambda(lambda x: K.clip(x, 1e-6, None) ** 3)(q_inputs)\n",
        "    q_outputs1 = keras.layers.GlobalAveragePooling1D()(q_inputs)\n",
        "    q_outputs1 = keras.layers.Dropout(dropout)(q_outputs1 ** (1./3))\n",
        "\n",
        "    category_emb = keras.layers.Embedding(input_dim=len(category_encoder.mapper)+1, output_dim=32)(input_category)\n",
        "    category_emb = keras.layers.SpatialDropout1D(0.1)(category_emb)\n",
        "\n",
        "    host_emb = keras.layers.Embedding(input_dim=len(host_encoder.mapper)+2, output_dim=32)(input_host)\n",
        "    host_emb = keras.layers.SpatialDropout1D(0.1)(host_emb)\n",
        "\n",
        "    features_dense = keras.layers.concatenate([category_emb, host_emb], axis=1)\n",
        "    features_dense = keras.layers.Flatten()(features_dense)\n",
        "\n",
        "    dense = keras.layers.concatenate([q_outputs1, features_dense])\n",
        "\n",
        "    if multi_target:\n",
        "        # outputs = [keras.layers.Lambda(transfer)(keras.layers.Dense(1, activation='sigmoid')(dense)) for _ in range(num_targets)]\n",
        "        outputs = [keras.layers.Dense(1, activation='sigmoid')(dense) for _ in range(num_targets)]\n",
        "        loss_func = [custom_loss]*num_targets\n",
        "\n",
        "    else:\n",
        "        # outputs = keras.layers.Lambda(transfer)(keras.layers.Dense(num_targets, activation='sigmoid')(dense))\n",
        "        outputs = keras.layers.Dense(num_targets, activation='sigmoid')(dense)\n",
        "        loss_func = ['binary_crossentropy']\n",
        "    # model = keras.Model([q_in, q2_in, q3_in], outputs)\n",
        "    model = keras.Model([q_in, q2_in, q3_in, input_category, input_host], outputs)\n",
        "\n",
        "\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=epochs,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=loss_func,\n",
        "        optimizer=AdamWarmup(\n",
        "            decay_steps=decay_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            lr=lr,\n",
        "            min_lr=MIN_LR,\n",
        "            ),\n",
        "        # optimizer = optimizer_lamb.LAMB(learning_rate=lr)\n",
        "        )\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N56RysQWJ6R1",
        "colab_type": "text"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWLYgjzR5Tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1627)\n",
        "def compute_spearmanr(trues, preds):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(spearmanr(col_trues, col_pred).correlation)\n",
        "    # rho_df = pd.DataFrame(dict(col=cols, rhos=rhos))\n",
        "    return np.nanmean(rhos)#, rho_df\n",
        "\n",
        "class IntervalEval(keras.callbacks.Callback):\n",
        "    def __init__(self, save_model, rho_init=-1, multi_target=True):\n",
        "        global NUM_EPOCHS\n",
        "        super().__init__()\n",
        "        self.rho = []\n",
        "        self.model_weights = []\n",
        "        self.save_model = save_model\n",
        "        self.multi_target = multi_target\n",
        "        self.rho_init = rho_init\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        val_pred = self.model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "        if self.multi_target:\n",
        "            val_pred = np.array(val_pred).squeeze().T\n",
        "        nunique = [i for i in range(val_pred.shape[1]) if len(np.unique(val_pred[:, i]))==1]\n",
        "        if len(nunique)>0:\n",
        "            print(f'{len(nunique)} cols with monotonous preds!')\n",
        "        score = compute_spearmanr(val_pred, val_y)\n",
        "        print('Spearman - {:.5f}'.format(score))\n",
        "        self.rho.append(score)\n",
        "        self.model_weights.append(self.model.get_weights())\n",
        "        # if score > self.rho:\n",
        "        #     self.rho = score\n",
        "        #     if self.stage==2:\n",
        "        #         print('--Save Model--')\n",
        "        #         self.model.save_weights('drive/My Drive/GoogleQA/Models/{:}/{:}-{:}-{:}.h5'.format(save_path, self.save_model, i, self.stage))\n",
        "        #     elif self.stage==1:\n",
        "        #         print('--Tmp Save--')\n",
        "        #         self.model.save_weights('model.h5')\n",
        "    def on_train_end(self, logs=None):\n",
        "        \"\"\"\n",
        "        Weighted average of weights from two best epochs\n",
        "\n",
        "        \"\"\"\n",
        "        rho_selected = np.argsort(self.rho)[-2:]\n",
        "        print(f'Best epoch {rho_selected[1]+1} and second best epoch {rho_selected[0]+1} selected')\n",
        "        swa_weights = [x * 0.3 + y * 0.7 for x, y in zip(self.model_weights[rho_selected[0]], self.model_weights[rho_selected[1]])]\n",
        "        self.model.set_weights(swa_weights)\n",
        "        val_pred = self.model.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "        if self.multi_target:\n",
        "            val_pred = np.array(val_pred).squeeze().T\n",
        "        nunique = [i for i in range(val_pred.shape[1]) if len(np.unique(val_pred[:, i]))==1]\n",
        "        if len(nunique)>0:\n",
        "            print(f'{len(nunique)} cols with monotonous preds!')\n",
        "        score = compute_spearmanr(val_pred, val_y)\n",
        "        print('SWD Model Spearman - {:.5f}'.format(score))\n",
        "        if score>self.rho_init:\n",
        "            self.rho_init = score\n",
        "            print('Save SWD Weights')\n",
        "            self.model.save_weights('drive/My Drive/GoogleQA/Models/{:}/{:}-{:}.h5'.format(save_path, self.save_model, i))\n",
        "        del self.rho, self.model_weights, swa_weights\n",
        "        gc.collect()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeg_K-UP6J4m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### QA combined Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH88F-DJMsTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=0)\n",
        "idx = [x for x in kf.split(train, groups=train.question_title)]\n",
        "multi_target = True\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx[7:], 8):\n",
        "    keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    tr = label_wt(tr, target_col)\n",
        "    tr_x, tr_aux, tr_y = convert_data(tr, model='qa', targets=target_col+col_wt)\n",
        "    val_x, val_aux, val_y = convert_data(val, model='qa', targets=target_col)\n",
        "\n",
        "    if sum(val[q_col].nunique()==1)>0:\n",
        "        print(f'\\n{sum(val[q_col].nunique()==1)} with Monotonous Label Detected!\\n')\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_aux, tr_y)), multi_target=multi_target)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux)), branch='valid', multi_target=multi_target)\n",
        "    \n",
        "    # stage 1 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model, multi_target=multi_target)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(target_col), \n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "    rho_stage1 = ieval.rho_init\n",
        "    gc.collect()\n",
        "\n",
        "    # stage 2 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model, multi_target=multi_target, rho_init=rho_stage1)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(target_col), \n",
        "        lr=1e-4, \n",
        "        epochs=4, \n",
        "        bert_trainable=False, \n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.load_weights('drive/My Drive/GoogleQA/Models/{:}/{:}-{:}.h5'.format(save_path, save_model, i))\n",
        "\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=4,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFFeB5SqFq8R",
        "colab_type": "text"
      },
      "source": [
        "### Q Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERdw3DrtFtJi",
        "colab_type": "code",
        "outputId": "a060c573-3117-4f0e-9517-86f31538d887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "kf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
        "idx_q = [x for x in kf.split(train, groups=train.question_title)]\n",
        "multi_target = False\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx_q[:1], 1):\n",
        "    keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    tr = q_avg(tr)\n",
        "    # tr = label_wt(tr, q_col)\n",
        "    # tr = data_aug(tr, model='q')\n",
        "    tr_x, tr_aux, tr_y = convert_data(tr, model='q', targets=q_col)\n",
        "    val_x, val_aux, val_y = convert_data(val, model='q', targets=q_col)\n",
        "\n",
        "    if sum(val[q_col].nunique()==1)>0:\n",
        "        print(f'\\n{sum(val[q_col].nunique()==1)} with Monotonous Label Detected!\\n')\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_aux, tr_y)), multi_target=multi_target)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux)), branch='valid', multi_target=multi_target)\n",
        "    \n",
        "    # stage 1 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model+'_q', multi_target=multi_target)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(q_col), \n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval]\n",
        "    )   \n",
        "    rho_stage1 = ieval.rho_init\n",
        "\n",
        "    # stage 2 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model+'_q', multi_target=multi_target, rho_init=rho_stage1)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(q_col), \n",
        "        lr=1e-4, \n",
        "        epochs=4, \n",
        "        bert_trainable=False, \n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.load_weights('drive/My Drive/GoogleQA/Models/{:}/{:}-{:}.h5'.format(save_path, save_model+'_q', i))\n",
        "    # model.load_weights('model.h5')\n",
        "\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=4,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 22/3175 [00:00<00:14, 218.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3175/3175 [00:13<00:00, 241.71it/s]\n",
            "100%|██████████| 1216/1216 [00:05<00:00, 232.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train for 794 steps\n",
            "Epoch 1/3\n",
            "304/304 [==============================] - 23s 75ms/step\n",
            "Spearman - 0.40149\n",
            "794/794 [==============================] - 248s 312ms/step - loss: 0.4119\n",
            "Epoch 2/3\n",
            "304/304 [==============================] - 21s 71ms/step\n",
            "Spearman - 0.42111\n",
            "794/794 [==============================] - 215s 271ms/step - loss: 0.3619\n",
            "Epoch 3/3\n",
            "162/794 [=====>........................] - ETA: 2:35 - loss: 0.3387"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCQmVtED95wF",
        "colab_type": "text"
      },
      "source": [
        "### A Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKbm-b6z97Nv",
        "colab_type": "code",
        "outputId": "f815e4eb-277d-4caf-d056-0fc44532b06e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        }
      },
      "source": [
        "kf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
        "idx_a = [x for x in kf.split(train_a, groups=train_a.question_title)]\n",
        "multi_target = True\n",
        "\n",
        "for i, (tr_idx, val_idx) in enumerate(idx_a[:1], 1):\n",
        "    keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train_a.loc[tr_idx], train_a.loc[val_idx]\n",
        "    tr = label_wt(tr, a_col)\n",
        "    # tr = data_aug(tr, model='a')\n",
        "    tr_x, tr_aux, tr_y = convert_data(tr, model='a', targets=a_col+a_col_wt)\n",
        "    val_x, val_aux, val_y = convert_data(val, model='a', targets=a_col)\n",
        "\n",
        "    if sum(val[a_col].nunique()==1)>0:\n",
        "        print(f'\\n{sum(val[a_col].nunique()==1)} with Monotonous Label Detected!\\n')\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_aux, tr_y)), multi_target=multi_target)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux)), branch='valid', multi_target=multi_target)\n",
        "    \n",
        "    # stage 1 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model+'_a', multi_target=multi_target)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(a_col),\n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval],\n",
        "    )   \n",
        "    rho_stage1 = ieval.rho_init\n",
        "    gc.collect()\n",
        "\n",
        "    # stage 2 fine tunning\n",
        "    ieval = IntervalEval(save_model=save_model+'_a', multi_target=multi_target, rho_init=rho_stage1)\n",
        "    model = model_build(\n",
        "        len_train=len(tr), \n",
        "        num_targets=len(a_col), \n",
        "        lr=1e-4, \n",
        "        epochs=4, \n",
        "        bert_trainable=False, \n",
        "        multi_target=multi_target,\n",
        "        dropout=0.1,\n",
        "        )\n",
        "    model.load_weights('drive/My Drive/GoogleQA/Models/{:}/{:}-{:}.h5'.format(save_path, save_model+'_a', i))\n",
        "    # model.load_weights('model.h5')\n",
        "\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=4,\n",
        "        callbacks = [ieval],\n",
        "    )\n",
        "\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 1216 steps\n",
            "Epoch 1/3\n",
            "304/304 [==============================] - 30s 100ms/step\n",
            "9 cols with monotonous preds!\n",
            "Spearman - nan\n",
            "1216/1216 [==============================] - 466s 383ms/step - loss: 5.0742 - lambda_loss: 0.5737 - lambda_1_loss: 0.5337 - lambda_2_loss: 0.4440 - lambda_3_loss: 0.3874 - lambda_4_loss: 0.7006 - lambda_5_loss: 0.6472 - lambda_6_loss: 0.4977 - lambda_7_loss: 0.6625 - lambda_8_loss: 0.6274\n",
            "Epoch 2/3\n",
            "  3/304 [..............................] - ETA: 29sBest epoch 2 and second best epoch 1 selected\n",
            " 29/304 [=>............................] - ETA: 26s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_start\u001b[0;34m(self, model, callbacks, use_samples, verbose, mode)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_successful_loop_finish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0da33215735c>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b2bb33223ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mieval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )  \n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_start\u001b[0;34m(self, model, callbacks, use_samples, verbose, mode)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;31m# End of all epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_end_hook\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;34m\"\"\"Helper function for on_{train|test|predict}_end methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \"\"\"\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0da33215735c>\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mswa_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho_selected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrho_selected\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswa_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBPNQxQdmjZh",
        "colab_type": "text"
      },
      "source": [
        "##oof prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWNNEfR8qnG5",
        "colab_type": "code",
        "outputId": "8882d9c4-5df5-4fdb-91fe-7ab717def820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "# oof-prediction\n",
        "multi_target = True\n",
        "model_qa = model_build(len_train=len(train), num_targets=len(target_col), bert_trainable=False, multi_target=multi_target)\n",
        "model_q = model_build(len_train=len(train), num_targets=len(q_col), bert_trainable=False, multi_target=multi_target)\n",
        "model_a = model_build(len_train=len(train), num_targets=len(a_col), bert_trainable=False, multi_target=multi_target)\n",
        "oof_pred_qa = train[['qa_id']+target_col].copy()\n",
        "oof_pred_qa[target_col] = 0\n",
        "oof_pred_q = train_q[['qa_id']+q_col].copy()\n",
        "oof_pred_q[q_col] = 0\n",
        "oof_pred_a = train_a[['qa_id']+a_col].copy()\n",
        "oof_pred_a[a_col] = 0\n",
        "\n",
        "kf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=0)\n",
        "idx_qa = [x for x in kf.split(train, groups=train.question_title)]\n",
        "for i, (tr_idx, val_idx) in enumerate(idx_qa, 1):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    val_x, val_aux, val_y = convert_data(val, 'qa', targets=target_col+col_wt)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux, val_y)), branch='valid', multi_target=multi_target)\n",
        "    model_qa.load_weights('drive/My Drive/GoogleQA/Models/{}/{}-{}.h5'.format(save_path, save_model, i))\n",
        "    val_pred_qa = model_qa.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    oof_pred_qa.loc[val_idx, target_col] += np.array(val_pred_qa).squeeze().T / 2\n",
        "\n",
        "    score = compute_spearmanr(val_y, np.array(val_pred_qa).squeeze().T)\n",
        "    print('\\nQA - Spearman - {:.5f}\\n'.format(score))\n",
        "\n",
        "\n",
        "kf = RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
        "idx_q = [x for x in kf.split(train, groups=train.question_title)]\n",
        "for i, (tr_idx, val_idx) in enumerate(idx_q, 1):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    # val = add_host_category(val)\n",
        "    val_x, val_aux, val_y = convert_data(val, 'q', targets=q_col+q_col_wt)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux, val_y)), branch='valid', multi_target=multi_target)\n",
        "    model_q.load_weights('drive/My Drive/GoogleQA/Models/{}/{}-{}.h5'.format(save_path, save_model+'_q', i))\n",
        "    val_pred_q = model_q.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    oof_pred_q.loc[val_idx, q_col] += np.array(val_pred_q).squeeze().T# / 2\n",
        "\n",
        "    score = compute_spearmanr(val_y, np.array(val_pred_q).squeeze().T)\n",
        "    print('\\nQ - Spearman - {:.5f}\\n'.format(score))\n",
        "\n",
        "idx_a = [x for x in kf.split(train, groups=train.question_title)]\n",
        "for i, (tr_idx, val_idx) in enumerate(idx_a, 1):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    val_x, val_aux, val_y = convert_data(val, 'a', targets=a_col+a_col_wt)\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux, val_y)), branch='valid', multi_target=multi_target)\n",
        "    model_a.load_weights('drive/My Drive/GoogleQA/Models/{}/{}-{}.h5'.format(save_path, save_model+'_a', i))\n",
        "    val_pred_a = model_a.predict_generator(valid_D.__iter__(), len(valid_D), verbose=1)\n",
        "    oof_pred_a.loc[val_idx, a_col] += np.array(val_pred_a).squeeze().T\n",
        "    \n",
        "    score = compute_spearmanr(val_y, np.array(val_pred_a).squeeze().T)\n",
        "    print('\\nA - Spearman - {:.5f}\\n'.format(score))\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "oof_pred = oof_pred_qa[target_col].values * 0.4 + np.concatenate((oof_pred_q[q_col].values, oof_pred_a[a_col].values), axis=1) * 0.6\n",
        "np.save('drive/My Drive/GoogleQA/Data/train_oof_pred_bbu10.npy', oof_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 24/717 [00:00<00:03, 226.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 717/717 [00:03<00:00, 222.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 15s 82ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/717 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q - Spearman - 0.39017\n",
            "\n",
            "\n",
            "Fold - 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 717/717 [00:03<00:00, 213.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 13s 75ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/717 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q - Spearman - 0.40762\n",
            "\n",
            "\n",
            "Fold - 3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 717/717 [00:03<00:00, 221.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 14s 78ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/716 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q - Spearman - 0.41647\n",
            "\n",
            "\n",
            "Fold - 4\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 716/716 [00:03<00:00, 233.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "179/179 [==============================] - 13s 72ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/716 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Q - Spearman - 0.40732\n",
            "\n",
            "\n",
            "Fold - 5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 716/716 [00:03<00:00, 230.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "179/179 [==============================] - 13s 73ms/step\n",
            "\n",
            "Q - Spearman - 0.40488\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JudYQut5qdbJ",
        "colab_type": "text"
      },
      "source": [
        "## oof-prediction hyperparameterize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnmddugkiwX",
        "colab_type": "text"
      },
      "source": [
        "### Just use \"90\" transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCqts6GIuZGA",
        "colab_type": "code",
        "outputId": "58c183a8-f54b-4f17-8108-668abf1b1a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "compute_spearmanr(oof_pred_[a_col].values, train_a[a_col].values)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3678945323743401"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cegkGSpSHCx",
        "colab_type": "code",
        "outputId": "33ef0980-9435-48ff-dc70-03b3b28ff32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# bbu3: 0.42130\n",
        "# bbu4: 0.43406\n",
        "# bbu5: 0.42747\n",
        "# bbu6: 0.43264\n",
        "# bbu9: 0.43900\n",
        "# bbu4 + bbu9: 0.44547\n",
        "# lindada*0.4 + bbu9*0.6: 0.44867\n",
        "oof_pred4 = pd.read_csv('drive/My Drive/GoogleQA/Data/train_oof_pred_bbu4.csv')\n",
        "oof_pred9 = pd.read_csv('drive/My Drive/GoogleQA/Data/train_oof_pred_bbu9.csv')\n",
        "oof_pred = oof_pred4[target_col] *0.4 + oof_pred9[target_col] *0.6\n",
        "score_qa = compute_spearmanr(np.floor(oof_pred_qa[target_col].values*90)/90, train[target_col].values)\n",
        "score_q = compute_spearmanr(np.floor(oof_pred_q[q_col].values*90)/90, train_q[q_col].values)\n",
        "score_a = compute_spearmanr(np.floor(oof_pred_a[a_col].values*90)/90, train_a[a_col].values)\n",
        "# print('Total Score: {:.5f}'.format(score_q*0.7+score_a*0.3))\n",
        "\n",
        "# just using \"90\" transformation\n",
        "scores = []\n",
        "for col, col_trues, col_pred in zip(target_col, train[target_col].values.T, (np.floor(oof_pred_qa[target_col].values *90)/90).T):\n",
        "    corr = spearmanr(col_trues, col_pred).correlation\n",
        "    print('{} - {:.5f}'.format(col, corr))\n",
        "    scores.append(corr)\n",
        "hyper1_qa_combined = pd.DataFrame(dict(col = target_col, score1 = scores))\n",
        "\n",
        "\n",
        "scores = []\n",
        "for col, col_trues, col_pred in zip(q_col, train[q_col].values.T, (np.floor(oof_pred_q[q_col].values *90)/90).T):\n",
        "    corr = spearmanr(col_trues, col_pred).correlation\n",
        "    print('{} - {:.5f}'.format(col, corr))\n",
        "    scores.append(corr)\n",
        "hyper1_q = pd.DataFrame(dict(col=q_col, score1=scores))\n",
        "\n",
        "scores = []\n",
        "for col, col_trues, col_pred in zip(a_col, train[a_col].values.T, (np.floor(oof_pred_a[a_col].values *90)/90).T):\n",
        "    corr = spearmanr(col_trues, col_pred).correlation\n",
        "    print('{} - {:.5f}'.format(col, corr))\n",
        "    scores.append(corr)\n",
        "print(np.mean(scores))\n",
        "hyper1_a = pd.DataFrame(dict(col=a_col, score1=scores))\n",
        "hyper1_qa = pd.concat([hyper1_q, hyper1_a], sort=False, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question_asker_intent_understanding - 0.38051\n",
            "question_body_critical - 0.64965\n",
            "question_conversational - 0.45531\n",
            "question_expect_short_answer - 0.31746\n",
            "question_fact_seeking - 0.38686\n",
            "question_has_commonly_accepted_answer - 0.44411\n",
            "question_interestingness_others - 0.35485\n",
            "question_interestingness_self - 0.49944\n",
            "question_multi_intent - 0.60302\n",
            "question_not_really_a_question - 0.13203\n",
            "question_opinion_seeking - 0.49888\n",
            "question_type_choice - 0.75529\n",
            "question_type_compare - 0.40765\n",
            "question_type_consequence - 0.22527\n",
            "question_type_definition - 0.48513\n",
            "question_type_entity - 0.50119\n",
            "question_type_instructions - 0.79027\n",
            "question_type_procedure - 0.36777\n",
            "question_type_reason_explanation - 0.68575\n",
            "question_type_spelling - 0.24185\n",
            "question_well_written - 0.53197\n",
            "answer_helpful - 0.23650\n",
            "answer_level_of_information - 0.43432\n",
            "answer_plausible - 0.16032\n",
            "answer_relevance - 0.19304\n",
            "answer_satisfaction - 0.33071\n",
            "answer_type_instructions - 0.76870\n",
            "answer_type_procedure - 0.29403\n",
            "answer_type_reason_explanation - 0.68688\n",
            "answer_well_written - 0.18471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w3-YFHgklQL",
        "colab_type": "text"
      },
      "source": [
        "### Use lower (to 0) or upper (to 1) resetting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJXnyWxJgag6",
        "colab_type": "code",
        "outputId": "ac985d40-2edf-45b2-efae-52ec739baf52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# bbu3: 0.44169\n",
        "# bbu4: 0.45424\n",
        "# bbu5: 0.45451\n",
        "# bbu6: 0.45252\n",
        "# bbu7: 0.45296\n",
        "# bbu9: 0.45732\n",
        "# bbu4 + bbu9: 0.46861 (LB: 0.457)\n",
        "# bbu4*0.4 + bbu9 * 0.6: 0.47113\n",
        "# lindada*0.4 + bbu9*0.6: 0.46791\n",
        "# bbu10 qa combined: 0.46153 (LB: 0.448)\n",
        "\n",
        "def f_min(params, pred, train, col):\n",
        "    tmp = pred.copy()[col].values\n",
        "    if params['choice'] == 'low':\n",
        "        loweridx = tmp.argsort()[:params['pct']]\n",
        "        tmp[loweridx] = 0\n",
        "    else:\n",
        "        upperidx = tmp.argsort()[-params['pct']:]\n",
        "        tmp[upperidx] = 1\n",
        "    score = spearmanr(train[col].values, tmp).correlation\n",
        "    return {'loss': -round(score, 5), 'status': STATUS_OK}\n",
        "\n",
        "score, choice, pct = [], [], []\n",
        "for col in target_col:\n",
        "    corrparams = {\n",
        "    'pct': hp.randint('pct', len(oof_pred_qa)),\n",
        "    'choice': hp.choice('choice', ['low', 'up'])\n",
        "    }\n",
        "    f = partial(f_min, pred=oof_pred_qa, train=train, col=col)\n",
        "    trials = Trials()\n",
        "    best = fmin(f, corrparams, tpe.suggest, 100, rstate=np.random.RandomState(0), trials=trials, show_progressbar=False)\n",
        "    print('{} - {} - {} - pct: {:.3f}'.format(\n",
        "        col, \n",
        "        -trials.best_trial['result']['loss'],\n",
        "        ['low', 'up'][best['choice']],\n",
        "        best['pct']/len(oof_pred_qa),\n",
        "        # best['upper']/len(oof_pred)\n",
        "        ))\n",
        "    score.append(-trials.best_trial['result']['loss'])\n",
        "    pct.append(round(best['pct']/len(oof_pred_qa), 3))\n",
        "    choice.append(['low', 'up'][best['choice']])\n",
        "hyper2_qa_combined = pd.DataFrame(dict(col = target_col, pct = pct, choice=choice, score2 = score))\n",
        "\n",
        "\n",
        "score, choice, pct = [], [], []\n",
        "for col in q_col:\n",
        "    corrparams = {\n",
        "    'pct': hp.randint('pct', len(oof_pred_q)),\n",
        "    'choice': hp.choice('choice', ['low', 'up'])\n",
        "    }\n",
        "    f = partial(f_min, pred=oof_pred_q, train=train_q, col=col)\n",
        "    trials = Trials()\n",
        "    best = fmin(f, corrparams, tpe.suggest, 100, rstate=np.random.RandomState(0), trials=trials, show_progressbar=False)\n",
        "    print('{} - {} - {} - pct: {:.3f}'.format(\n",
        "        col, \n",
        "        -trials.best_trial['result']['loss'],\n",
        "        ['low', 'up'][best['choice']],\n",
        "        best['pct']/len(oof_pred_q),\n",
        "        # best['upper']/len(oof_pred)\n",
        "        ))\n",
        "    score.append(-trials.best_trial['result']['loss'])\n",
        "    pct.append(round(best['pct']/len(oof_pred_q), 3))\n",
        "    choice.append(['low', 'up'][best['choice']])\n",
        "hyper2_q = pd.DataFrame(dict(col = q_col, pct = pct, choice=choice, score2 = score))\n",
        "hyper2_q.score2.mean()\n",
        "\n",
        "score, choice, pct = [], [], []\n",
        "for col in a_col:\n",
        "    corrparams = {\n",
        "    'pct': hp.randint('pct', len(oof_pred_a)),\n",
        "    'choice': hp.choice('choice', ['low', 'up'])\n",
        "    }\n",
        "    f = partial(f_min, pred=oof_pred_a, train=train_a, col=col)\n",
        "    trials = Trials()\n",
        "    best = fmin(f, corrparams, tpe.suggest, 100, rstate=np.random.RandomState(0), trials=trials, show_progressbar=False)\n",
        "    print('{} - {} - {} - pct: {:.3f}'.format(\n",
        "        col, \n",
        "        -trials.best_trial['result']['loss'],\n",
        "        ['low', 'up'][best['choice']],\n",
        "        best['pct']/len(oof_pred_a),\n",
        "        # best['upper']/len(oof_pred)\n",
        "        ))\n",
        "    score.append(-trials.best_trial['result']['loss'])\n",
        "    pct.append(round(best['pct']/len(oof_pred_a), 3))\n",
        "    choice.append(['low', 'up'][best['choice']])\n",
        "\n",
        "print(np.mean(score))\n",
        "\n",
        "hyper2_a = pd.DataFrame(dict(col = a_col, pct = pct, choice=choice, score2 = score))\n",
        "hyper2_qa = pd.concat([hyper2_q, hyper2_a], sort=False, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question_asker_intent_understanding - 0.38184 - low - pct: 0.005\n",
            "question_body_critical - 0.65004 - low - pct: 0.005\n",
            "question_conversational - 0.51796 - low - pct: 0.881\n",
            "question_expect_short_answer - 0.31758 - low - pct: 0.005\n",
            "question_fact_seeking - 0.3871 - low - pct: 0.005\n",
            "question_has_commonly_accepted_answer - 0.49328 - up - pct: 0.794\n",
            "question_interestingness_others - 0.36194 - low - pct: 0.495\n",
            "question_interestingness_self - 0.51426 - low - pct: 0.620\n",
            "question_multi_intent - 0.61359 - low - pct: 0.555\n",
            "question_not_really_a_question - 0.14429 - low - pct: 0.928\n",
            "question_opinion_seeking - 0.49871 - low - pct: 0.005\n",
            "question_type_choice - 0.77419 - low - pct: 0.484\n",
            "question_type_compare - 0.53411 - low - pct: 0.928\n",
            "question_type_consequence - 0.26367 - low - pct: 0.953\n",
            "question_type_definition - 0.63317 - low - pct: 0.928\n",
            "question_type_entity - 0.61702 - low - pct: 0.881\n",
            "question_type_instructions - 0.79948 - up - pct: 0.354\n",
            "question_type_procedure - 0.36789 - low - pct: 0.005\n",
            "question_type_reason_explanation - 0.68667 - low - pct: 0.187\n",
            "question_type_spelling - 0.45173 - low - pct: 0.999\n",
            "question_well_written - 0.5323 - low - pct: 0.055\n",
            "answer_helpful - 0.24013 - up - pct: 0.036\n",
            "answer_level_of_information - 0.4353 - low - pct: 0.005\n",
            "answer_plausible - 0.16317 - up - pct: 0.203\n",
            "answer_relevance - 0.19625 - up - pct: 0.281\n",
            "answer_satisfaction - 0.33203 - low - pct: 0.005\n",
            "answer_type_instructions - 0.77115 - up - pct: 0.317\n",
            "answer_type_procedure - 0.29456 - low - pct: 0.055\n",
            "answer_type_reason_explanation - 0.68693 - up - pct: 0.069\n",
            "answer_well_written - 0.18545 - low - pct: 0.055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czO2o18alP1a",
        "colab_type": "text"
      },
      "source": [
        "### Use n-binning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTzolfJeWgXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bbu3: 0.43042\n",
        "# bbu4: 0.44542\n",
        "\n",
        "def f1_min(params, col):\n",
        "    tmp = oof_pred.copy()[col].values\n",
        "    paramlist = [params['p'+str(i)] for i in range(len(params))]\n",
        "    bins = np.percentile(tmp, paramlist)\n",
        "    tmp = np.digitize(tmp, np.sort(bins)) / len(bins)\n",
        "    score = spearmanr(train[col].values, tmp).correlation\n",
        "    return {'loss': -round(score, 5), 'status': STATUS_OK}\n",
        "\n",
        "def f2_min(params, col):\n",
        "    cutoff = {'p'+str(i): hp.randint('p'+str(i), 101) for i in range(params['num_bin'])}\n",
        "    f = partial(f1_min, col=col)\n",
        "    trials = Trials()\n",
        "    best = fmin(f, cutoff, tpe.suggest, 100, rstate=np.random.RandomState(0), trials=trials, \n",
        "                show_progressbar=False)\n",
        "    # print('{} - {}'.format(col, -trials.best_trial['result']['loss']))\n",
        "    score = -trials.best_trial['result']['loss']\n",
        "    return {'loss': -round(score, 5), 'status': STATUS_OK, 'cutoff': list(best.values())}\n",
        "\n",
        "score, cutoffs = [], []\n",
        "for col in target_col:\n",
        "    numbin = {'num_bin': hp.randint('num_bin', 19)+2}\n",
        "    f = partial(f2_min, col=col)\n",
        "    trials = Trials()\n",
        "    best = fmin(f, numbin, tpe.suggest, 10, rstate=np.random.RandomState(0), trials=trials, show_progressbar=False)\n",
        "    print('{} - {}'.format(col, -trials.best_trial['result']['loss']))\n",
        "    score.append(-trials.best_trial['result']['loss'])\n",
        "    cutoffs.append(trials.best_trial['result']['cutoff'])\n",
        "\n",
        "print(np.mean(score))\n",
        "hyper3 = pd.DataFrame(dict(col = target_col, cutoffs_pct = cutoffs, score3 = score))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmmArJLLknTN",
        "colab_type": "text"
      },
      "source": [
        "### Combine \"90\" transform and 0/1 resetting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-09vXszbrtM8",
        "colab_type": "code",
        "outputId": "1916f085-609f-4ce5-c751-cfc4b8f43013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# bbu 3: 0.44462\n",
        "# bbu 4: 0.45486 (LB: 0.444)\n",
        "# bbu 5: 0.44808 (LB: 0.441)\n",
        "# bbu 6: 0.4539\n",
        "# bbu 7: 0.45297\n",
        "# bbu 9: 0.45735 (LB: 0.451)\n",
        "# bbu 9 *0.6 + lindada *0.4: 0.46795\n",
        "# bbu 10_qa_combined: 0.46153\n",
        "hyper = hyper1_qa_combined.merge(hyper2_qa_combined, on='col')#.merge(hyper3, on='col')\n",
        "hyper.to_csv('drive/My Drive/GoogleQA/Models/bbu10/hyper_bbu10_qa_combined.csv', index=False)\n",
        "hyper['score'] = hyper[['score1', 'score2']].max(axis=1)\n",
        "hyper['score'].mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615320591221413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oL7Sk2GZpeM",
        "colab_type": "text"
      },
      "source": [
        "### use hyper_bbu4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2XmA2oWYokZ",
        "colab_type": "code",
        "outputId": "0861575f-cde6-4a7e-8d2d-85da8c0024a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "hyper = pd.read_csv('drive/My Drive/GoogleQA/Data/hyper_bbu4.csv')\n",
        "col1 = hyper.loc[hyper[['score1', 'score2']].max(axis=1) == hyper.score1, 'col'].tolist()\n",
        "col2 = hyper.loc[hyper[['score1', 'score2']].max(axis=1) == hyper.score2, 'col'].tolist()\n",
        "pred = np.copy(oof_pred)\n",
        "\n",
        "for col in col1:\n",
        "    colidx = target_col.index(col)\n",
        "    pred[:, colidx] = (pred[:, colidx]//(1/90))/90\n",
        "\n",
        "for col in col2:\n",
        "    if hyper.loc[hyper.col==col, 'pct'].values == 1:\n",
        "        print(f'{col} - minus 0.005')\n",
        "        pct = hyper.loc[hyper.col==col, 'pct'] - 0.005\n",
        "    elif hyper.loc[hyper.col==col, 'pct'].values == 0:\n",
        "        print(f'{col} - plus 0.005')\n",
        "        pct = hyper.loc[hyper.col==col, 'pct'] + 0.005\n",
        "    else:\n",
        "        print(f'{col} - use original')\n",
        "        pct = hyper.loc[hyper.col==col, 'pct']\n",
        "        \n",
        "    changerow = int(len(test) * pct)\n",
        "    colidx = target_col.index(col)\n",
        "    \n",
        "    if hyper.loc[hyper.col==col, 'choice'].values =='low':\n",
        "        rowidx = pred[:, colidx].argsort()[:changerow]\n",
        "        pred[rowidx, colidx] = 0\n",
        "    elif hyper.loc[hyper.col==col, 'choice'].values =='up':\n",
        "        rowidx = pred[:, colidx].argsort()[-changerow:]\n",
        "        pred[rowidx, colidx] = 1\n",
        "    else:\n",
        "        print('Wrong!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question_asker_intent_understanding - use original\n",
            "question_conversational - use original\n",
            "question_has_commonly_accepted_answer - use original\n",
            "question_interestingness_others - use original\n",
            "question_interestingness_self - use original\n",
            "question_multi_intent - use original\n",
            "question_not_really_a_question - use original\n",
            "question_opinion_seeking - plus 0.005\n",
            "question_type_choice - use original\n",
            "question_type_compare - use original\n",
            "question_type_consequence - use original\n",
            "question_type_definition - use original\n",
            "question_type_entity - use original\n",
            "question_type_instructions - use original\n",
            "question_type_procedure - use original\n",
            "question_type_reason_explanation - use original\n",
            "question_type_spelling - minus 0.005\n",
            "question_well_written - use original\n",
            "answer_helpful - plus 0.005\n",
            "answer_plausible - use original\n",
            "answer_relevance - use original\n",
            "answer_satisfaction - plus 0.005\n",
            "answer_type_instructions - use original\n",
            "answer_type_procedure - plus 0.005\n",
            "answer_type_reason_explanation - plus 0.005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4pu4zbKpBAW",
        "colab_type": "text"
      },
      "source": [
        "## test prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR1RU0X9DDAJ",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKXUgWqPDBkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", top_k=100, aug_p=0.3)\n",
        "def row_aug(data_df):\n",
        "    q_id, a_id = [], []\n",
        "    for col in target_col:\n",
        "        a = data_df[col].value_counts(normalize=True)\n",
        "        b = a[a < 0.01].index.tolist()\n",
        "        c = data_df.loc[data_df[col].isin(b), 'qa_id'].tolist()\n",
        "        if 'question' in col:\n",
        "            q_id.extend(c)\n",
        "        else:\n",
        "            a_id.extend(c)\n",
        "    return list(set(q_id)), list(set(a_id))\n",
        "\n",
        "def data_aug(data_df, model):\n",
        "    q_id, a_id = row_aug(data_df)\n",
        "    if model == 'q':\n",
        "        for row in tqdm(q_id):    \n",
        "            qt = data_df.loc[data_df.qa_id==row, 'question_title'].to_string(index=False)\n",
        "            qb = data_df.loc[data_df.qa_id==row, 'question_body'].to_string(index=False)\n",
        "            # for _ in range(1):\n",
        "            tmp = data_df[data_df.qa_id==row].copy()\n",
        "            tmp['question_title'] = aug.augment(qt)\n",
        "            tmp['question_body'] = aug.augment(qb)\n",
        "            data_df = pd.concat([data_df, tmp], ignore_index=True)\n",
        "    else:\n",
        "        for row in tqdm(a_id):    \n",
        "            an = data_df.loc[data_df.qa_id==row, 'answer'].to_string(index=False)\n",
        "            # for _ in range(1):\n",
        "            tmp = data_df[data_df.qa_id==row].copy()\n",
        "            tmp['answer'] = aug.augment(an)\n",
        "            data_df = pd.concat([data_df, tmp], ignore_index=True)\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2J1yS8HpCrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = pd.read_csv('drive/My Drive/GoogleQA/Data/test_oof_pred.csv')\n",
        "hyper = pd.read_csv('drive/My Drive/GoogleQA/Data/hyper.csv')\n",
        "pred = test_pred[target_col].copy().values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRYN8QsepVkN",
        "colab_type": "code",
        "outputId": "513a2ec1-1c99-4725-db98-ff7f542e4fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "col1 = hyper.loc[hyper[['score1', 'score2', 'score3']].max(axis=1) == hyper.score1, 'col'].tolist()\n",
        "col2 = hyper.loc[hyper[['score1', 'score2', 'score3']].max(axis=1) == hyper.score2, 'col'].tolist()\n",
        "col3 = hyper.loc[hyper[['score1', 'score2', 'score3']].max(axis=1) == hyper.score3, 'col'].tolist()\n",
        "\n",
        "\n",
        "for col in col1:\n",
        "    colidx = target_col.index(col)\n",
        "    pred[:, colidx] = (pred[:, colidx]//(1/90))/90\n",
        "\n",
        "for col in col2:\n",
        "    if hyper.loc[hyper.col==col, 'lower'].values == 1:\n",
        "        pct = hyper.loc[hyper.col==col, 'lower'] - 0.01\n",
        "    else:\n",
        "        pct = hyper.loc[hyper.col==col, 'lower']\n",
        "    changerow = int(len(test) * pct)\n",
        "    colidx = target_col.index(col)\n",
        "    rowidx = pred[:, colidx].argsort()[:changerow]\n",
        "    pred[rowidx, colidx] = 0\n",
        "    \n",
        "for col in col3:\n",
        "    cutoffs_pct = [int(x) for x in hyper.loc[hyper.col==col, 'cutoffs_pct'].strip('[]').split(',')]\n",
        "    colidx = target_col.index(col)\n",
        "    bins = np.percentile(pred[:, colidx], list(set(cutoffs_pct)))\n",
        "    pred[:, colidx] = np.digitize(pred[:, colidx], np.sort(bins)) / len(bins)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['question_body_critical', 'answer_level_of_information']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdbE1aufyFmS",
        "colab_type": "text"
      },
      "source": [
        "## Experimental engineered features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A8YF2gNyEL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def aux_features(data_df):\n",
        "#     data_df[\"n_question_body\"] = data_df[\"question_body\"].apply(lambda x: len(x.strip())) / 10000\n",
        "#     data_df[\"n_question_title\"] = data_df[\"question_title\"].apply(lambda x: len(x.strip())) / 100\n",
        "#     data_df[\"n_answer\"] = data_df[\"answer\"].apply(lambda x: len(x.strip())) / 10000\n",
        "#     data_df[\"n_word_question_body\"] = data_df[\"question_body\"].apply(lambda x: len(x.split())) / 100\n",
        "#     data_df[\"n_word_question_title\"] = data_df[\"question_title\"].apply(lambda x: len(x.split())) / 100\n",
        "#     data_df[\"n_word_answer\"] = data_df[\"answer\"].apply(lambda x: len(x.split())) / 100\n",
        "#     data_df[\"is_question_text_in_title\"] = data_df[\"question_title\"].apply(lambda x: float(\n",
        "#         x.split()[0] in [\"Do\", \"Can\", \"What\", \"How\", \"Is\", \"Which\", \"When\", \"Where\", \"can\", \"what\", \"how\", \"is\",\n",
        "#                             \"which\", \"What's\", \"does\", \"Does\", \"Are\", \"are\", \"Should\", \"Will\"] or x[-1] in [\"?\"]))\n",
        "#     data_df[\"same_auther\"] = (data_df[\"question_user_name\"] == data_df[\"answer_user_name\"]).astype(float)\n",
        "#     data_df[\"question_title_selection\"] = (data_df[\"question_title\"].apply(lambda x: \"or\" in x)).astype(float)\n",
        "\n",
        "#     for i in tqdm(range(len(data_df))):\n",
        "#         indirect = 0\n",
        "#         question_count = 0\n",
        "#         reason_explanation_words = 0\n",
        "#         choice_words = 0\n",
        "#         doc = nlp(data_df.loc[i, 'question_body'])\n",
        "#         for sent in doc.sents:\n",
        "#             if '?' in sent.text and '?' == sent.text[-1]:\n",
        "#                 question_count += 1  # -> question_multi_intent\n",
        "#             for token in sent:\n",
        "#                 if token.text.lower() == 'why':  # question_type_reason_explanation e.g index->102\n",
        "#                     reason_explanation_words += 1\n",
        "#                 elif token.text.lower() == 'or':\n",
        "#                     choice_words += 1  # question_type_choice\n",
        "#         if question_count == 0:\n",
        "#             indirect = 1\n",
        "#         data_df.loc[i, 'indirect'] = indirect\n",
        "#         data_df.loc[i, 'question_count'] = question_count\n",
        "#         data_df.loc[i, 'reason_explanation_words'] = reason_explanation_words\n",
        "#         data_df.loc[i, 'choice_words'] = choice_words\n",
        "  \n",
        "#     data_df['host'] = data_df.host.apply(lambda x: x.split('.')[0])\n",
        "#     data_df['question_title'] = data_df['category']+' '+data_df['host']+' '+data_df['question_title']\n",
        "\n",
        "#     return data_df\n",
        "\n",
        "# train, test = aux_features(train), aux_features(test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}