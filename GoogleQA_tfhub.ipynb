{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleQA.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acmilannesta/Bert-embedding/blob/master/GoogleQA_tfhub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82bieSpLcuc",
        "colab_type": "code",
        "outputId": "4eb56e37-5dee-4e1f-9b33-6b5bb647a066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 11 23:06:46 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.36       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JErzYVOxLtWo",
        "colab_type": "code",
        "outputId": "22cfd362-982c-44a4-b57e-e67a684fa905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "# bert uncased base\n",
        "!wget https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1?tf-hub-format=compressed -O bbu\n",
        "!mkdir bert_base_uncased\n",
        "!tar -xzf bbu -C bert_base_uncased\n",
        "\n",
        "\n",
        "# bert cased base\n",
        "!wget https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1?tf-hub-format=compressed -O bbc\n",
        "!mkdir bert_base_cased\n",
        "!tar -xzf bbc -C bert_base_cased\n",
        "\n",
        "# albert base\n",
        "!wget https://tfhub.dev/google/albert_base/1?tf-hub-format=compressed -O ab\n",
        "!mkdir albert_base\n",
        "!tar -xzf ab -C albert_base\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-11 23:06:55--  https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1?tf-hub-format=compressed\n",
            "Resolving tfhub.dev (tfhub.dev)... 74.125.203.139, 74.125.203.113, 74.125.203.101, ...\n",
            "Connecting to tfhub.dev (tfhub.dev)|74.125.203.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-12_H-768_A-12/1.tar.gz [following]\n",
            "--2019-12-11 23:07:00--  https://storage.googleapis.com/tfhub-modules/tensorflow/bert_en_uncased_L-12_H-768_A-12/1.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.128, 2404:6800:4008:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405800905 (387M) [application/x-tar]\n",
            "Saving to: ‘bbu’\n",
            "\n",
            "bbu                 100%[===================>] 387.00M  79.3MB/s    in 5.4s    \n",
            "\n",
            "2019-12-11 23:07:06 (72.0 MB/s) - ‘bbu’ saved [405800905/405800905]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBrwK8dwLyw8",
        "colab_type": "code",
        "outputId": "d6beef71-076c-4f74-fe28-f69ce320aa78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaDm4NWvLp_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(1, '/content/drive/My Drive/GoogleQA')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, gc\n",
        "import codecs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.backend import set_session\n",
        "# from keras.callbacks import Callback\n",
        "# from keras.models import Model, load_model\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GroupKFold, KFold\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "# from albert_tokenization import FullTokenizer\n",
        "from tokenizer import Tokenizer\n",
        "from warmup_v2 import AdamWarmup, calc_train_steps\n",
        "tf.get_logger().setLevel('WARNING') \n",
        "# from keras_bert import load_trained_model_from_checkpoint, Tokenizer, AdamWarmup, calc_train_steps, get_custom_objects\n",
        "# from hyperopt import fmin, hp, tpe, STATUS_OK, Trials\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xGojs4dL6S4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'drive/My Drive/GoogleQA/Data/'\n",
        "train = pd.read_csv(data_path+'train.csv')\n",
        "test = pd.read_csv(data_path+'test.csv')\n",
        "sub = pd.read_csv(data_path+'sample_submission.csv')\n",
        "\n",
        "target_col = train.columns.tolist()[11:42]\n",
        "train['question'] = train['question_title'] + ' ' + train['question_body']\n",
        "test['question'] = test['question_title'] + ' ' + test['question_body']\n",
        "\n",
        "def aux_build(df):\n",
        "    aux = pd.get_dummies(df['category'], drop_first=True)\n",
        "    aux['stackexchange'] = df.host.str.contains('stackexchange').astype('int32')\n",
        "    aux['stackoverflow'] = df.host.str.contains('stackoverflow').astype('int32')\n",
        "    return aux\n",
        "train_aux, test_aux = aux_build(train), aux_build(test)\n",
        "# aux1 = pd.get_dummies(df_aux['category'], drop_first=True)\n",
        "# aux2 = pd.get_dummies(df_aux['host'].apply(lambda x: x.split('.')[0]), drop_first=True)\n",
        "# aux = pd.concat([aux1, aux2], 1)\n",
        "# train_aux = pd.concat([train, aux.iloc[:len(train), ]], 1)\n",
        "# test_aux =  pd.concat([test, aux.iloc[len(train):, ]], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPkHK5JvMJpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN = 512 #@param {type:\"slider\", min:128, max:512, step:32}\n",
        "BATCH_SIZE = 4 #@param {type:'slider', min:1, max:32, step:1}\n",
        "NUM_EPOCHS = 3\n",
        "NUM_CLASSES = 30\n",
        "LR = 5e-5\n",
        "MIN_LR = 0\n",
        "model_path = 'bert_base_uncased' #@param ['bert_base_uncased', 'bert_base_cased', 'albert_base']\n",
        "save_model = 'bbu'\n",
        "# OUTPUT_TRAIN = 'train_bert_ipredcv1415_oof.csv'\n",
        "# OUTPUT_TEST = 'test_bert_large.npy'\n",
        "# model_path = 'uncased_L-12_H-768_A-12' #@param ['uncased_L-12_H-768_A-12', 'wwm_uncased_L-24_H-1024_A-16', 'uncased_L-24_H-1024_A-16']\n",
        "# target_q_col = train.columns.tolist()[11:32]\n",
        "# target_a_col = train.columns.tolist()[32:42]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MknUuh8pFpq0",
        "colab_type": "text"
      },
      "source": [
        "## Bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2llbNDMhVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(os.path.join(model_path, 'assets/vocab.txt'), 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "tokenizer = Tokenizer(token_dict)\n",
        "\n",
        "def convert_data(data_df, branch='training'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    global MAXLEN\n",
        "    global target_col\n",
        "    ids, segments = [], []\n",
        "    # q_title, q_body, answer = [], [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        a, b = tokenizer.encode(data_df.loc[i, 'question'], data_df.loc[i, 'answer'])\n",
        "        ids.append(a)\n",
        "        segments.append(b)                 \n",
        "        # q_title_ids, _ = tokenizer.encode(data_df.loc[i, 'question'])\n",
        "        # q_title.append(q_title_ids)\n",
        "        # q_body_ids, _ = tokenizer.encode(data_df.loc[i, 'answer'])\n",
        "        # q_body.append(q_body_ids)\n",
        "    if branch == 'training':\n",
        "        targets = data_df[target_col]\n",
        "        # return [q_title, q_body], np.array(targets)\n",
        "        return [ids, segments], np.array(targets)\n",
        "    else:\n",
        "        return [ids, segments]\n",
        "        # return [q_title, q_body]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6elLMNFvAc",
        "colab_type": "text"
      },
      "source": [
        "## Albert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4zB4cIjMjFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = FullTokenizer(\n",
        "    vocab_file=model_path+'/assets/30k-clean.vocab', \n",
        "    do_lower_case=True, \n",
        "    spm_model_file=model_path + '/assets/30k-clean.model')\n",
        "\n",
        "def convert_data(data_df, branch='training'):\n",
        "    data_df = data_df.reset_index(drop=True)\n",
        "    global tokenizer\n",
        "    global MAXLEN\n",
        "\n",
        "    ids, segments = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        id = tokenizer.tokenize(data_df.loc[i, 'question']) + tokenizer.tokenize(data_df.loc[i, 'answer'])\n",
        "        id = tokenizer.convert_tokens_to_ids(id)\n",
        "        segment = [0] * len(data_df.loc[i, 'question']) + [1] * len(data_df.loc[i, 'answer'])\n",
        "        ids.append(id)\n",
        "        segments.append(segment)\n",
        "    if branch == 'training':\n",
        "        targets = data_df[target_col]\n",
        "        # return [q_title, q_body], np.array(targets)\n",
        "        return [ids, segments], np.array(targets)\n",
        "    else:\n",
        "        return [ids, segments]\n",
        "        # return [q_title, q_body]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7qwb8ZMlbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Data Generator\"\"\"\n",
        "def seq_padding(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = min(max(L), MAXLEN)\n",
        "    return np.array([np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x[:ML] for x in X])\n",
        "\n",
        "def get_masks(X, padding=0):\n",
        "    L = [len(x) for x in X]\n",
        "    ML = min(max(L), MAXLEN)\n",
        "    return np.array([np.concatenate([[1]*len(x), [padding] * (ML - len(x))]) if len(x) < ML else [1]*ML for x in X])\n",
        "\n",
        "\n",
        "class data_generator:\n",
        "    def __init__(self, data, batch_size=BATCH_SIZE, branch='train'):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.branch = branch\n",
        "        # self.q_a = q_a\n",
        "        self.steps = len(self.data) // self.batch_size\n",
        "        if len(self.data) % self.batch_size != 0:\n",
        "            self.steps += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if self.branch == 'train':\n",
        "                np.random.shuffle(self.data)\n",
        "            for i in range(self.steps):\n",
        "                d = self.data[i * self.batch_size: (i + 1) * self.batch_size]\n",
        "                X1 = seq_padding([x[0] for x in d])\n",
        "                X2 = get_masks([x[0] for x in d])    \n",
        "                X3 = seq_padding([x[1] for x in d])\n",
        "                # X3 = np.zeros_like(X1)\n",
        "\n",
        "                # X4 = seq_padding([x[1] for x in d])\n",
        "                # X5 = get_masks([x[1] for x in d])    \n",
        "                # X6 = np.zeros_like(X4)\n",
        "\n",
        "                # X7 = seq_padding([x[2] for x in d])\n",
        "                # X8 = get_masks([x[2] for x in d])    \n",
        "                # X9 = np.zeros_like(X7)\n",
        "                aux = np.array([x[2] for x in d])\n",
        "                if self.branch == 'test':\n",
        "                    # aux = np.array([x[3] for x in d])\n",
        "                    yield [X1, X2, X3, aux]\n",
        "                    # yield [X1, X2, X3, X4, X5, X6]\n",
        "                else:\n",
        "                    Y = np.array([x[3] for x in d])\n",
        "                    # aux = np.array([x[4] for x in d])\n",
        "                    yield [X1, X2, X3, aux], Y\n",
        "                    # yield [X1, X2, X3, X4, X5, X6], Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvVEWF62IdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_build(len_train, model='albert'):\n",
        "    global NUM_CLASSES\n",
        "    global BATCH_SIZE\n",
        "    global NUM_EPOCHS\n",
        "    global MIN_LR\n",
        "    global LR\n",
        "    global MAXLEN\n",
        "    global model_path\n",
        "    global train_aux\n",
        "    # bert_model = load_trained_model_from_checkpoint(\n",
        "    #     os.path.join(model_path, 'bert_config.json'),\n",
        "    #     os.path.join(model_path, 'bert_model.ckpt'),\n",
        "    #     seq_len = MAXLEN,\n",
        "    #     trainable=True,\n",
        "    # )\n",
        "    \n",
        "\n",
        "    q_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_input_word_ids\")\n",
        "    q2_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_input_masks\")\n",
        "    q3_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"q_segment_ids\")\n",
        "    aux_in = keras.layers.Input(shape=(train_aux.shape[1],), name=\"aux\")\n",
        "    # qb_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"qb_input_word_ids\")\n",
        "    # qb2_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"qb_input_masks\")\n",
        "    # qb3_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"qb_segment_ids\")\n",
        "\n",
        "    # a_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"a_input_word_ids\")\n",
        "    # a2_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"a_input_masks\")\n",
        "    # a3_in = keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"a_segment_ids\")\n",
        "\n",
        "    # _, qb_inputs  = bert_layer([qb_in, qb2_in, qb3_in])\n",
        "    # qb_outputs = keras.layers.Lambda(lambda x: x[:, 0])(qb_inputs)\n",
        "\n",
        "    # _, a_inputs  = bert_layer([a_in, a2_in, a3_in])\n",
        "    # a_outputs = keras.layers.Lambda(lambda x: x[:, 0])(a_inputs)\n",
        "    if model=='bert':\n",
        "        bert_layer = hub.KerasLayer(model_path, trainable=True)\n",
        "        _, q_inputs  = bert_layer([q_in, q2_in, q3_in])\n",
        "        # q_outputs = keras.layers.Lambda(lambda x: x[:, 0])(q_inputs)\n",
        "        q_outputs = keras.layers.GlobalAveragePooling1D()(q_inputs)\n",
        "        dense = keras.layers.Dropout(0.2)(q_outputs)\n",
        "        del bert_layer\n",
        "        gc.collect()\n",
        "\n",
        "    if model=='albert':\n",
        "        albert_module = hub.Module(model_path, trainable=True)\n",
        "        albert_inputs = dict(input_ids=q_in, input_mask=q2_in, segment_ids=q3_in)\n",
        "        albert_outputs = albert_module(albert_inputs, signature=\"tokens\", as_dict=True)[\"sequence_output\"]\n",
        "        sess = tf.Session()\n",
        "        graph = tf.get_default_graph()\n",
        "        dense = keras.layers.Lambda(lambda x: x[:, 0])(albert_outputs)\n",
        "        del albert_module\n",
        "        gc.collect()\n",
        "    \n",
        "    dense = keras.layers.concatenate([dense, aux_in])\n",
        "    outputs = keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dense)\n",
        "    model = keras.Model([q_in, q2_in, q3_in, aux_in], outputs)\n",
        "\n",
        "    decay_steps, warmup_steps = calc_train_steps(\n",
        "        len_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=NUM_EPOCHS,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "    #     optimizer=keras.optimizers.Adam(LR),\n",
        "        optimizer=AdamWarmup(\n",
        "            decay_steps=decay_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            lr=LR,\n",
        "            min_lr=MIN_LR,\n",
        "            ))\n",
        "    # )\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N56RysQWJ6R1",
        "colab_type": "text"
      },
      "source": [
        "## train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWLYgjzR5Tk",
        "colab_type": "code",
        "outputId": "4ecb2890-e901-432d-925c-9c1ded11eddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_x = convert_data(test, branch='testing')\n",
        "pred = np.zeros((len(test), NUM_CLASSES))\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "idx = [x for x in kf.split(train)]\n",
        "\n",
        "def compute_spearmanr(trues, preds):\n",
        "    rhos = []\n",
        "    for col_trues, col_pred in zip(trues.T, preds.T):\n",
        "        rhos.append(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
        "    return np.nanmean(rhos)\n",
        "\n",
        "class IntervalEval(keras.callbacks.Callback):\n",
        "    def __init__(self, val_data, test_data, label):\n",
        "        global NUM_EPOCHS\n",
        "        global save_model\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "        self.val_data = val_data\n",
        "        self.test_data = test_data\n",
        "        self.label = label\n",
        "        self.total_epochs = NUM_EPOCHS\n",
        "        self.test_pred = []\n",
        "        self.score = []\n",
        "        self.save_model = save_model\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # if epoch == 3:\n",
        "        val_pred = self.model.predict_generator(self.val_data.__iter__(), len(self.val_data), verbose=1)\n",
        "        score = compute_spearmanr(self.label, val_pred)\n",
        "        self.score.append(score)\n",
        "        print('Spearman - {:.5f}'.format(score))\n",
        "        if epoch>=self.total_epochs-2:\n",
        "            print('--Save Model--')\n",
        "            self.model.save('drive/My Drive/GoogleQA/Models/{:}-{:}-{:}.h5'.format(self.save_model, i, epoch+1))\n",
        "        #     print(\"Testing Pred\")\n",
        "        #     test_pred = self.model.predict_generator(self.test_data.__iter__(), len(self.test_data), verbose=1)\n",
        "        #     self.test_pred.append(test_pred)\n",
        "        # if epoch==self.total_epochs-1:\n",
        "        #     if self.score[-2]<=self.score[-1]:\n",
        "        #         self.test_pred = self.test_pred[0] * 0.3 + self.test_pred[1] * 0.7\n",
        "        #     else:\n",
        "        #         self.test_pred = self.test_pred[0] * 0.7 + self.test_pred[1] * 0.3"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 476/476 [00:01<00:00, 313.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH88F-DJMsTR",
        "colab_type": "code",
        "outputId": "312ba55d-4581-4d5f-c1fb-0b97cd8032ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "for i, (tr_idx, val_idx) in enumerate(idx[1:], 2):\n",
        "    print('\\nFold - {:}\\n'.format(i))\n",
        "    tr, val = train.loc[tr_idx], train.loc[val_idx]\n",
        "    tr_aux, val_aux = train_aux.loc[tr_idx], train_aux.loc[val_idx]\n",
        "    tr_x, tr_y = convert_data(tr)\n",
        "    val_x, val_y = convert_data(val)\n",
        "    \n",
        "    model = model_build(len(tr), model='bert')\n",
        "\n",
        "    train_D = data_generator(list(zip(tr_x[0], tr_x[1], tr_aux.values, tr_y)))\n",
        "    valid_D = data_generator(list(zip(val_x[0], val_x[1], val_aux.values, val_y)), branch='valid')\n",
        "    test_D = data_generator(list(zip(test_x[0], test_x[1], test_aux.values)), branch='test')\n",
        "    ieval = IntervalEval(val_data=valid_D, label=val_y, test_data=test_D)\n",
        "    model.fit_generator(\n",
        "        train_D.__iter__(),\n",
        "        steps_per_epoch=len(train_D),\n",
        "        epochs=NUM_EPOCHS,\n",
        "        callbacks = [ieval]\n",
        "    )\n",
        "    # pred+ = model.predict_generator(test_D.__iter__(), len(test_D), verbose=1) / kf.get_n_splits()\n",
        "    # train_aug.loc[val_idx, 'oof_pred'] = np.argmax(oof_pred, 1)\n",
        "    # print('oof - {:} f1_score - {:.4f}'.format(i, spearmanr(val_y, np.argmax(oof_pred, 1), average='weighted')))\n",
        "\n",
        "    # pred+= ieval.test_pred / kf.get_n_splits()\n",
        "    # np.save(OUTPUT_TEST, pred)\n",
        "    # s3.upload_file(Filename=OUTPUT_TEST, Bucket='acmilannesta', Key='large/'+OUTPUT_TEST)\n",
        "\n",
        "    # np.save('drive/My Drive/GoogleQA/pred', pred)\n",
        "    # model.save('drive/My Drive/GoogleQA/Models/bertbase-{:}.h5'.format(i))\n",
        "\n",
        "    del model, train_D, valid_D, test_D, ieval, tr, val, tr_x, tr_y, val_x, val_y\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 28/4863 [00:00<00:17, 276.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 2\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4863/4863 [00:21<00:00, 224.85it/s]\n",
            "100%|██████████| 1216/1216 [00:03<00:00, 333.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/3\n",
            "304/304 [==============================] - 38s 125ms/step\n",
            "Spearman - 0.36789\n",
            "1216/1216 [==============================] - 505s 415ms/step - loss: 0.4044\n",
            "Epoch 2/3\n",
            "304/304 [==============================] - 28s 93ms/step\n",
            "Spearman - 0.38569\n",
            "--Save Model--\n",
            "1216/1216 [==============================] - 435s 358ms/step - loss: 0.3631\n",
            "Epoch 3/3\n",
            "304/304 [==============================] - 28s 93ms/step\n",
            "Spearman - 0.39556\n",
            "--Save Model--\n",
            "1216/1216 [==============================] - 429s 353ms/step - loss: 0.3387\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 26/4863 [00:00<00:19, 251.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold - 3\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4863/4863 [00:15<00:00, 314.59it/s]\n",
            "100%|██████████| 1216/1216 [00:10<00:00, 112.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 167/1216 [===>..........................] - ETA: 13:47 - loss: 0.4911"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL0ON33qlBBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub[target_col] = pred\n",
        "sub.to_csv('drive/My Drive/GoogleQA/Submissions/sub_1.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}